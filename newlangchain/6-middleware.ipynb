{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52b224",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Middleware - Processing Messages Before and After LLM Calls (LangChain 1.0+)\n# =============================================================================\n\"\"\"\n=====================================================================\nUnderstanding Middleware in LangChain\n=====================================================================\n\nMiddleware intercepts and processes messages before they reach the LLM\nand after the LLM responds. Think of it like HTTP middleware in web apps.\n\nWhy Use Middleware?\n-------------------\n1. Context Management - Summarize long conversations to fit context windows\n2. Logging - Track all LLM interactions\n3. Caching - Store and retrieve common responses\n4. Filtering - Block certain types of content\n5. Transformation - Modify messages before/after LLM\n\nMiddleware Flow:\n----------------\n    User Input\n         â”‚\n         â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚  Middleware Layer   â”‚  â† Pre-process (summarize, filter, transform)\n    â”‚  (before LLM)       â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚       LLM           â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚  Middleware Layer   â”‚  â† Post-process (log, cache, transform)\n    â”‚  (after LLM)        â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\n    Final Response\n\nKey Middleware Types (LangChain 1.0+):\n--------------------------------------\n- SummarizationMiddleware - Summarizes old messages when context gets long\n- Custom middleware using RunnableLambda\n\nUpdated for LangChain 1.0+ (2025-2026)\n\"\"\"\n\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\nos.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\nos.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n\nprint(\"âœ… Environment configured for middleware examples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde858d0",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Custom Middleware with RunnableLambda (LangChain 1.0+)\n# =============================================================================\n\"\"\"\nBuilding Custom Middleware\n--------------------------\nIn LangChain 1.0+, you can create middleware-like behavior using:\n1. RunnableLambda for transformations\n2. Callbacks for logging/monitoring\n3. Chain composition for pre/post processing\n\nThis approach gives you full control over the message flow.\n\"\"\"\n\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\nfrom langchain_core.messages import SystemMessage, HumanMessage, AIMessage\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom typing import List, Dict, Any\nimport time\n\n# Initialize model\nmodel = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0.7)\n\n# =============================================================================\n# Middleware 1: Logging Middleware\n# =============================================================================\n\"\"\"\nLogs all inputs and outputs for debugging/monitoring.\n\"\"\"\n\ndef logging_middleware(func_name: str):\n    \"\"\"Create a logging wrapper for any function.\"\"\"\n    def log_input(x):\n        print(f\"ðŸ“¥ [{func_name}] Input: {str(x)[:100]}...\")\n        return x\n    \n    def log_output(x):\n        print(f\"ðŸ“¤ [{func_name}] Output: {str(x)[:100]}...\")\n        return x\n    \n    return RunnableLambda(log_input), RunnableLambda(log_output)\n\n# Create logging wrappers\nlog_in, log_out = logging_middleware(\"LLM_CALL\")\n\n# Chain with logging\nlogged_model = log_in | model | log_out\n\nprint(\"=\" * 60)\nprint(\"Middleware 1: Logging Example\")\nprint(\"=\" * 60)\n\nresponse = logged_model.invoke([HumanMessage(content=\"What is 2+2?\")])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40077528",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Middleware 2: Content Filtering Middleware\n# =============================================================================\n\"\"\"\nFilters or transforms content before it reaches the LLM.\nUseful for:\n- Removing sensitive information\n- Blocking certain topics\n- Sanitizing user input\n\"\"\"\n\n# List of blocked words/topics (example)\nBLOCKED_TOPICS = [\"password\", \"credit card\", \"ssn\", \"social security\"]\n\ndef content_filter(messages: List) -> List:\n    \"\"\"\n    Pre-processing middleware that filters sensitive content.\n    \"\"\"\n    filtered_messages = []\n    \n    for msg in messages:\n        content = msg.content.lower()\n        \n        # Check for blocked topics\n        blocked = [topic for topic in BLOCKED_TOPICS if topic in content]\n        \n        if blocked:\n            # Replace with warning message\n            print(f\"âš ï¸  Blocked topics detected: {blocked}\")\n            filtered_messages.append(\n                HumanMessage(content=\"[Content filtered - sensitive information removed]\")\n            )\n        else:\n            filtered_messages.append(msg)\n    \n    return filtered_messages\n\n# Create filter middleware\nfilter_middleware = RunnableLambda(content_filter)\n\n# Chain with filtering\nfiltered_model = filter_middleware | model\n\nprint(\"=\" * 60)\nprint(\"Middleware 2: Content Filtering Example\")\nprint(\"=\" * 60)\n\n# Test with safe content\nprint(\"\\nâœ… Safe content:\")\nsafe_response = filtered_model.invoke([\n    HumanMessage(content=\"What is the capital of France?\")\n])\nprint(f\"   Response: {safe_response.content[:100]}...\")\n\n# Test with blocked content\nprint(\"\\nâ›” Content with blocked topic:\")\nblocked_response = filtered_model.invoke([\n    HumanMessage(content=\"What is my credit card number?\")\n])\nprint(f\"   Response: {blocked_response.content[:100]}...\")"
  },
  {
   "cell_type": "code",
   "id": "sxk11vb0wac",
   "source": "# =============================================================================\n# Middleware 3: Message Trimming/Summarization\n# =============================================================================\n\"\"\"\nHandles long conversations by trimming or summarizing old messages.\nThis is crucial for staying within context window limits.\n\"\"\"\n\nfrom langchain_core.messages import trim_messages\n\ndef context_manager(max_messages: int = 10):\n    \"\"\"\n    Middleware that manages conversation context.\n    Keeps system message + last N messages.\n    \"\"\"\n    def trim_context(messages: List) -> List:\n        if len(messages) <= max_messages:\n            return messages\n        \n        # Separate system message from others\n        system_msgs = [m for m in messages if isinstance(m, SystemMessage)]\n        other_msgs = [m for m in messages if not isinstance(m, SystemMessage)]\n        \n        # Keep system + last (max_messages - 1) messages\n        trimmed = system_msgs + other_msgs[-(max_messages - len(system_msgs)):]\n        \n        print(f\"ðŸ“‹ Context trimmed: {len(messages)} â†’ {len(trimmed)} messages\")\n        return trimmed\n    \n    return RunnableLambda(trim_context)\n\n# Create context manager middleware\ncontext_middleware = context_manager(max_messages=6)\n\nprint(\"=\" * 60)\nprint(\"Middleware 3: Context Management Example\")\nprint(\"=\" * 60)\n\n# Simulate a long conversation\nlong_conversation = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n]\nfor i in range(10):\n    long_conversation.append(HumanMessage(content=f\"Question {i+1}\"))\n    long_conversation.append(AIMessage(content=f\"Answer {i+1}\"))\n\nprint(f\"\\nðŸ“Š Original conversation: {len(long_conversation)} messages\")\n\n# Apply trimming middleware\ntrimmed = context_middleware.invoke(long_conversation)\nprint(f\"ðŸ“Š After middleware: {len(trimmed)} messages\")\nprint(f\"\\nðŸ“‹ Kept messages:\")\nfor msg in trimmed:\n    msg_type = type(msg).__name__[:6]\n    print(f\"   [{msg_type}] {msg.content[:40]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f0uvdei4yqg",
   "source": "# =============================================================================\n# Middleware 4: Response Transformation Middleware\n# =============================================================================\n\"\"\"\nPost-processing middleware that transforms LLM responses.\nUseful for:\n- Formatting responses\n- Adding metadata\n- Extracting specific parts\n\"\"\"\n\ndef response_formatter(format_type: str = \"bullet\"):\n    \"\"\"\n    Post-processing middleware that formats responses.\n    \"\"\"\n    def format_response(response: AIMessage) -> AIMessage:\n        content = response.content\n        \n        if format_type == \"uppercase\":\n            # Convert to uppercase (simple example)\n            formatted = content.upper()\n        elif format_type == \"with_timestamp\":\n            # Add timestamp\n            import datetime\n            timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            formatted = f\"[{timestamp}]\\n{content}\"\n        elif format_type == \"word_count\":\n            # Add word count\n            word_count = len(content.split())\n            formatted = f\"{content}\\n\\n---\\nðŸ“Š Word count: {word_count}\"\n        else:\n            formatted = content\n        \n        return AIMessage(content=formatted)\n    \n    return RunnableLambda(format_response)\n\nprint(\"=\" * 60)\nprint(\"Middleware 4: Response Transformation Example\")\nprint(\"=\" * 60)\n\n# Create different formatters\ntimestamp_formatter = response_formatter(\"with_timestamp\")\nwordcount_formatter = response_formatter(\"word_count\")\n\n# Chain with formatting\nformatted_model = model | wordcount_formatter\n\nresponse = formatted_model.invoke([\n    HumanMessage(content=\"Explain what Python is in 2 sentences.\")\n])\n\nprint(f\"\\nðŸ“ Formatted Response:\")\nprint(response.content)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "p3wilg6zrzt",
   "source": "# =============================================================================\n# Middleware 5: Chaining Multiple Middleware\n# =============================================================================\n\"\"\"\nCombining Multiple Middleware Layers\n------------------------------------\nYou can chain multiple middleware together for complex processing.\nOrder matters: they execute left to right for input, right to left for output.\n\"\"\"\n\ndef timing_middleware():\n    \"\"\"Middleware that measures LLM call duration.\"\"\"\n    start_time = None\n    \n    def start_timer(x):\n        nonlocal start_time\n        start_time = time.time()\n        print(\"â±ï¸  Starting LLM call...\")\n        return x\n    \n    def end_timer(x):\n        duration = time.time() - start_time\n        print(f\"â±ï¸  LLM call completed in {duration:.2f}s\")\n        return x\n    \n    return RunnableLambda(start_timer), RunnableLambda(end_timer)\n\ndef system_prompt_injector(system_content: str):\n    \"\"\"Middleware that ensures a system message is always present.\"\"\"\n    def inject(messages: List) -> List:\n        has_system = any(isinstance(m, SystemMessage) for m in messages)\n        if not has_system:\n            print(\"ðŸ’‰ Injecting system message\")\n            return [SystemMessage(content=system_content)] + messages\n        return messages\n    return RunnableLambda(inject)\n\nprint(\"=\" * 60)\nprint(\"Middleware 5: Chained Middleware Example\")\nprint(\"=\" * 60)\n\n# Create middleware components\ntimer_start, timer_end = timing_middleware()\nsystem_injector = system_prompt_injector(\"You are a concise assistant. Keep answers brief.\")\n\n# Build the full pipeline:\n# inject system â†’ filter â†’ start timer â†’ model â†’ end timer â†’ format\nfull_pipeline = (\n    system_injector \n    | filter_middleware \n    | timer_start \n    | model \n    | timer_end \n    | wordcount_formatter\n)\n\nprint(\"\\nðŸ”— Full middleware chain:\")\nprint(\"   system_injector â†’ filter â†’ timer_start â†’ model â†’ timer_end â†’ formatter\")\n\n# Test the full pipeline\nresponse = full_pipeline.invoke([\n    HumanMessage(content=\"What is machine learning?\")\n])\n\nprint(f\"\\nðŸ“ Final Response:\\n{response.content}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5makxfrwx5b",
   "source": "# =============================================================================\n# SummarizationMiddleware with create_agent (LangChain Agents)\n# =============================================================================\n\"\"\"\nBuilt-in SummarizationMiddleware\n--------------------------------\nLangChain provides SummarizationMiddleware for agents that automatically\nsummarizes old conversation turns when context gets long.\n\nThis is used with create_agent() and requires:\n- A checkpointer for state persistence\n- Configuration for when to trigger summarization\n\nNote: This requires langgraph for checkpointing functionality.\n\"\"\"\n\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\n\n# Note: This example shows the pattern. You need langgraph installed for checkpointing.\nprint(\"=\" * 60)\nprint(\"SummarizationMiddleware Pattern (for Agents)\")\nprint(\"=\" * 60)\n\nprint(\"\"\"\nThe SummarizationMiddleware is used with create_agent():\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\n\nagent = create_agent(\n    model=\"gpt-4o-mini\",\n    tools=[your_tools],\n    checkpointer=InMemorySaver(),  # Required for state\n    middleware=[\n        SummarizationMiddleware(\n            model=\"gpt-4o-mini\",          # Model for summarization\n            trigger=(\"tokens\", 4000),     # Trigger at 4000 tokens\n            keep=(\"messages\", 20),        # Keep last 20 messages\n        ),\n    ],\n)\n\n# Use with thread_id for conversation persistence\nconfig = {\"configurable\": {\"thread_id\": \"user_123\"}}\nresponse = agent.invoke({\"messages\": [HumanMessage(...)]}, config=config)\n```\n\nHow it works:\n1. Monitors conversation token count\n2. When trigger threshold is reached:\n   - Summarizes older messages\n   - Keeps recent messages intact\n   - Replaces old messages with summary\n3. Keeps conversation within context limits\n\"\"\")\n\n# =============================================================================\n# Practical Example: Conversation with Auto-Summarization\n# =============================================================================\n\"\"\"\nWithout the full agent setup, here's how to implement similar\nfunctionality using pure middleware:\n\"\"\"\n\ndef summarization_middleware(model, token_threshold: int = 1000):\n    \"\"\"\n    Middleware that summarizes old messages when context gets long.\n    \"\"\"\n    def maybe_summarize(messages: List) -> List:\n        # Simple token estimation (actual implementation would use tiktoken)\n        estimated_tokens = sum(len(m.content.split()) * 1.3 for m in messages)\n        \n        if estimated_tokens < token_threshold:\n            return messages\n        \n        print(f\"ðŸ“ Context too long (~{int(estimated_tokens)} tokens). Summarizing...\")\n        \n        # Separate system from conversation\n        system_msgs = [m for m in messages if isinstance(m, SystemMessage)]\n        conv_msgs = [m for m in messages if not isinstance(m, SystemMessage)]\n        \n        # Keep last 4 messages, summarize the rest\n        to_summarize = conv_msgs[:-4]\n        to_keep = conv_msgs[-4:]\n        \n        if to_summarize:\n            # Create summary of old messages\n            summary_prompt = f\"Summarize this conversation briefly:\\n\\n\"\n            for msg in to_summarize:\n                role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n                summary_prompt += f\"{role}: {msg.content}\\n\"\n            \n            summary = model.invoke([HumanMessage(content=summary_prompt)])\n            \n            # Create new message list\n            summary_msg = SystemMessage(\n                content=f\"[Previous conversation summary: {summary.content}]\"\n            )\n            \n            return system_msgs + [summary_msg] + to_keep\n        \n        return messages\n    \n    return RunnableLambda(maybe_summarize)\n\nprint(\"\\nðŸ“‹ Custom summarization middleware created\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "06oztgcyjyeo",
   "source": "# =============================================================================\n# Summary: Middleware in LangChain 1.0+\n# =============================================================================\n\"\"\"\n=====================================================================\nKEY TAKEAWAYS - Middleware in LangChain\n=====================================================================\n\n1. MIDDLEWARE CONCEPT:\n   -------------------\n   Middleware intercepts messages before/after LLM calls.\n   Use RunnableLambda to create custom middleware.\n\n2. COMMON MIDDLEWARE PATTERNS:\n   ---------------------------\n   # Logging\n   def log(x): print(x); return x\n   logged_chain = RunnableLambda(log) | model\n   \n   # Filtering\n   def filter(msgs): return [m for m in msgs if is_safe(m)]\n   filtered_chain = RunnableLambda(filter) | model\n   \n   # Transformation\n   def transform(response): return modify(response)\n   transformed_chain = model | RunnableLambda(transform)\n\n3. CHAINING MIDDLEWARE:\n   --------------------\n   pipeline = (\n       preprocessing_middleware\n       | model\n       | postprocessing_middleware\n   )\n\n4. BUILT-IN SUMMARIZATION:\n   ------------------------\n   from langchain.agents.middleware import SummarizationMiddleware\n   \n   agent = create_agent(\n       model=\"gpt-4o-mini\",\n       middleware=[\n           SummarizationMiddleware(\n               model=\"gpt-4o-mini\",\n               trigger=(\"tokens\", 4000),\n               keep=(\"messages\", 20),\n           ),\n       ],\n   )\n\n5. BEST PRACTICES:\n   ----------------\n   - Keep middleware focused (single responsibility)\n   - Log inputs/outputs for debugging\n   - Handle errors gracefully\n   - Consider performance impact\n   - Test middleware in isolation\n\nCommon Imports:\n---------------\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\nfrom langchain.agents.middleware import SummarizationMiddleware\n\n=====================================================================\n\"\"\"\n\nprint(\"=\" * 60)\nprint(\"Middleware Module Complete!\")\nprint(\"=\" * 60)\nprint(\"\"\"\nNext Topics to Explore:\n-----------------------\n1. 7-LCEL.ipynb - LangChain Expression Language deep dive\n2. 8-streaming.ipynb - Streaming and async patterns\n3. 9-RAG.ipynb - Retrieval Augmented Generation\n4. 10-agents.ipynb - Agent patterns with create_agent()\n\nMiddleware Use Cases:\n---------------------\n- Rate limiting and throttling\n- Caching frequent responses\n- Input sanitization\n- Response formatting\n- Analytics and monitoring\n- A/B testing different prompts\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}