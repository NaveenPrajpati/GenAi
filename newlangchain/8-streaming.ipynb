{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Streaming & Async - Real-Time LLM Responses (LangChain 1.0+)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "=====================================================================\n",
    "Streaming and Asynchronous Operations\n",
    "=====================================================================\n",
    "\n",
    "Why Streaming?\n",
    "--------------\n",
    "- Better UX: Users see responses as they're generated\n",
    "- Lower perceived latency: First token appears quickly\n",
    "- Memory efficient: Don't buffer entire response\n",
    "\n",
    "Why Async?\n",
    "----------\n",
    "- Handle multiple requests concurrently\n",
    "- Better performance in web applications\n",
    "- Non-blocking I/O operations\n",
    "\n",
    "LangChain Methods:\n",
    "------------------\n",
    "Sync:                    Async:\n",
    ".invoke(input)          .ainvoke(input)\n",
    ".batch([inputs])        .abatch([inputs])\n",
    ".stream(input)          .astream(input)\n",
    "                        .astream_events(input)\n",
    "\n",
    "Updated for LangChain 1.0+ (2025-2026)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "print(\"âœ… Environment configured for streaming examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde858d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Basic Streaming with .stream()\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    ".stream() Method\n",
    "----------------\n",
    "Returns an iterator that yields chunks as they're generated.\n",
    "Each chunk contains a small piece of the response.\n",
    "\n",
    "For chat models, chunks are AIMessageChunk objects.\n",
    "For chains with StrOutputParser, chunks are strings.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize model\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0.7)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Basic Streaming Example\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nðŸ“¤ Streaming response (token by token):\\n\")\n",
    "\n",
    "# Stream from model directly\n",
    "for chunk in model.stream([HumanMessage(content=\"Write a haiku about Python programming\")]):\n",
    "    # chunk.content contains the text piece\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\nâœ… Streaming complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Streaming with LCEL Chains\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Streaming Through Chains\n",
    "------------------------\n",
    "When you stream through a chain, the streaming happens\n",
    "at the model step and propagates through parsers.\n",
    "\n",
    "Chain: prompt â†’ model (streams) â†’ parser (passes chunks)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Streaming Through LCEL Chain\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a chain\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a creative storyteller.\"),\n",
    "    (\"human\", \"Write a very short story about {topic} in 3 sentences.\")\n",
    "])\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "print(\"\\nðŸ“– Streaming story about 'a robot learning to cook':\\n\")\n",
    "\n",
    "# Stream through the entire chain\n",
    "for chunk in chain.stream({\"topic\": \"a robot learning to cook\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\nâœ… Chain streaming complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Async Operations with asyncio\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Async Methods (a-prefix)\n",
    "------------------------\n",
    "All runnables support async versions:\n",
    "- await chain.ainvoke(input)\n",
    "- await chain.abatch([inputs])\n",
    "- async for chunk in chain.astream(input)\n",
    "\n",
    "Benefits:\n",
    "- Non-blocking: Other tasks can run while waiting for LLM\n",
    "- Concurrent requests: Handle multiple users simultaneously\n",
    "- Better performance in web frameworks (FastAPI, etc.)\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Async Operations Example\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "async def async_invoke_example():\n",
    "    \"\"\"Example of async invoke.\"\"\"\n",
    "    print(\"\\n1ï¸âƒ£ Async invoke (single request):\")\n",
    "    \n",
    "    result = await chain.ainvoke({\"topic\": \"a time-traveling cat\"})\n",
    "    print(f\"   {result[:100]}...\")\n",
    "    return result\n",
    "\n",
    "async def async_batch_example():\n",
    "    \"\"\"Example of async batch - process multiple inputs concurrently.\"\"\"\n",
    "    print(\"\\n2ï¸âƒ£ Async batch (multiple requests in parallel):\")\n",
    "    \n",
    "    topics = [\n",
    "        {\"topic\": \"a magic coffee machine\"},\n",
    "        {\"topic\": \"a talking houseplant\"},\n",
    "        {\"topic\": \"a detective dog\"},\n",
    "    ]\n",
    "    \n",
    "    # All three requests run concurrently!\n",
    "    results = await chain.abatch(topics)\n",
    "    \n",
    "    for i, (topic, result) in enumerate(zip(topics, results)):\n",
    "        print(f\"   Story {i+1} ({topic['topic']}): {result[:50]}...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "async def async_stream_example():\n",
    "    \"\"\"Example of async streaming.\"\"\"\n",
    "    print(\"\\n3ï¸âƒ£ Async streaming:\")\n",
    "    \n",
    "    print(\"   \", end=\"\")\n",
    "    async for chunk in chain.astream({\"topic\": \"a singing laptop\"}):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print()\n",
    "\n",
    "# Run all async examples\n",
    "async def main():\n",
    "    await async_invoke_example()\n",
    "    await async_batch_example()\n",
    "    await async_stream_example()\n",
    "\n",
    "# In Jupyter, use await directly\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Concurrent Async Requests\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Running Multiple LLM Calls Concurrently\n",
    "---------------------------------------\n",
    "Use asyncio.gather() to run multiple async calls at once.\n",
    "This is faster than sequential calls.\n",
    "\n",
    "Timing comparison:\n",
    "- Sequential: 3 calls Ã— 2s each = 6s total\n",
    "- Concurrent: 3 calls running together = ~2s total\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Concurrent vs Sequential Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "topics = [\n",
    "    {\"topic\": \"artificial intelligence\"},\n",
    "    {\"topic\": \"quantum computing\"},\n",
    "    {\"topic\": \"biotechnology\"},\n",
    "]\n",
    "\n",
    "# Sequential approach\n",
    "async def sequential_calls():\n",
    "    results = []\n",
    "    start = time.time()\n",
    "    \n",
    "    for topic in topics:\n",
    "        result = await chain.ainvoke(topic)\n",
    "        results.append(result)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    return results, elapsed\n",
    "\n",
    "# Concurrent approach using gather\n",
    "async def concurrent_calls():\n",
    "    start = time.time()\n",
    "    \n",
    "    # Create all tasks\n",
    "    tasks = [chain.ainvoke(topic) for topic in topics]\n",
    "    \n",
    "    # Run all concurrently\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    return results, elapsed\n",
    "\n",
    "# Compare both approaches\n",
    "async def compare():\n",
    "    print(\"\\nâ±ï¸ Sequential calls (one after another):\")\n",
    "    seq_results, seq_time = await sequential_calls()\n",
    "    print(f\"   Time: {seq_time:.2f}s\")\n",
    "    \n",
    "    print(\"\\nâš¡ Concurrent calls (all at once):\")\n",
    "    con_results, con_time = await concurrent_calls()\n",
    "    print(f\"   Time: {con_time:.2f}s\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Speedup: {seq_time/con_time:.1f}x faster with concurrency!\")\n",
    "\n",
    "await compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Streaming Events with astream_events()\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    ".astream_events() Method\n",
    "------------------------\n",
    "Provides detailed streaming events from ALL steps in a chain.\n",
    "Useful for:\n",
    "- Debugging complex chains\n",
    "- Progress tracking\n",
    "- Detailed logging\n",
    "\n",
    "Event Types:\n",
    "- on_chain_start, on_chain_end\n",
    "- on_llm_start, on_llm_stream, on_llm_end\n",
    "- on_tool_start, on_tool_end\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Streaming Events Example\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "async def stream_events_example():\n",
    "    print(\"\\nðŸ“Š Streaming events from chain:\\n\")\n",
    "    \n",
    "    async for event in chain.astream_events(\n",
    "        {\"topic\": \"a friendly alien\"},\n",
    "        version=\"v2\"  # Use v2 for latest event format\n",
    "    ):\n",
    "        kind = event[\"event\"]\n",
    "        \n",
    "        if kind == \"on_chain_start\":\n",
    "            print(f\"ðŸ”— Chain started: {event['name']}\")\n",
    "        \n",
    "        elif kind == \"on_chat_model_stream\":\n",
    "            # This is where tokens are streamed\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                print(content, end=\"\", flush=True)\n",
    "        \n",
    "        elif kind == \"on_chain_end\":\n",
    "            print(f\"\\n\\nâœ… Chain ended: {event['name']}\")\n",
    "\n",
    "await stream_events_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Callbacks for Monitoring\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Callback Handlers\n",
    "-----------------\n",
    "Callbacks let you hook into the execution lifecycle.\n",
    "Built-in handlers:\n",
    "- StdOutCallbackHandler - Print to console\n",
    "- StreamingStdOutCallbackHandler - Stream tokens to console\n",
    "\n",
    "You can also create custom handlers.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.outputs import LLMResult\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Custom Callback Handler Example\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class TokenCounterCallback(BaseCallbackHandler):\n",
    "    \"\"\"Custom callback that counts tokens and measures time.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.token_count = 0\n",
    "        self.start_time = None\n",
    "    \n",
    "    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs):\n",
    "        self.start_time = time.time()\n",
    "        print(f\"\\nðŸš€ LLM started\")\n",
    "    \n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        self.token_count += 1\n",
    "        print(token, end=\"\", flush=True)\n",
    "    \n",
    "    def on_llm_end(self, response: LLMResult, **kwargs):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        print(f\"\\n\\nðŸ“Š Stats: {self.token_count} tokens in {elapsed:.2f}s\")\n",
    "        print(f\"   Speed: {self.token_count/elapsed:.1f} tokens/second\")\n",
    "\n",
    "# Use the callback\n",
    "callback = TokenCounterCallback()\n",
    "\n",
    "# Invoke with callback\n",
    "result = model.invoke(\n",
    "    [HumanMessage(content=\"Explain what streaming means in 2 sentences.\")],\n",
    "    config={\"callbacks\": [callback]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FastAPI Integration Pattern\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Streaming with FastAPI\n",
    "----------------------\n",
    "Real-world pattern for streaming LLM responses in web apps.\n",
    "Uses Server-Sent Events (SSE) for real-time streaming.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FastAPI Streaming Pattern\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "# FastAPI streaming endpoint example:\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "app = FastAPI()\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "@app.post(\"/stream\")\n",
    "async def stream_response(query: str):\n",
    "    \n",
    "    async def generate():\n",
    "        async for chunk in model.astream([HumanMessage(content=query)]):\n",
    "            if chunk.content:\n",
    "                yield f\"data: {chunk.content}\\\\n\\\\n\"\n",
    "        yield \"data: [DONE]\\\\n\\\\n\"\n",
    "    \n",
    "    return StreamingResponse(\n",
    "        generate(),\n",
    "        media_type=\"text/event-stream\"\n",
    "    )\n",
    "\n",
    "# Frontend JavaScript:\n",
    "const eventSource = new EventSource('/stream?query=Hello');\n",
    "eventSource.onmessage = (event) => {\n",
    "    if (event.data === '[DONE]') {\n",
    "        eventSource.close();\n",
    "    } else {\n",
    "        document.getElementById('output').innerHTML += event.data;\n",
    "    }\n",
    "};\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Summary: Streaming & Async in LangChain 1.0+\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "=====================================================================\n",
    "KEY TAKEAWAYS - Streaming & Async\n",
    "=====================================================================\n",
    "\n",
    "1. STREAMING METHODS:\n",
    "   ------------------\n",
    "   # Sync streaming\n",
    "   for chunk in chain.stream(input):\n",
    "       print(chunk, end=\"\")\n",
    "   \n",
    "   # Async streaming\n",
    "   async for chunk in chain.astream(input):\n",
    "       print(chunk, end=\"\")\n",
    "\n",
    "2. ASYNC METHODS:\n",
    "   ---------------\n",
    "   result = await chain.ainvoke(input)    # Single async\n",
    "   results = await chain.abatch([inputs]) # Batch async\n",
    "\n",
    "3. CONCURRENT EXECUTION:\n",
    "   ----------------------\n",
    "   # Run multiple calls in parallel\n",
    "   tasks = [chain.ainvoke(inp) for inp in inputs]\n",
    "   results = await asyncio.gather(*tasks)\n",
    "\n",
    "4. STREAMING EVENTS:\n",
    "   ------------------\n",
    "   async for event in chain.astream_events(input, version=\"v2\"):\n",
    "       if event[\"event\"] == \"on_chat_model_stream\":\n",
    "           print(event[\"data\"][\"chunk\"].content)\n",
    "\n",
    "5. CALLBACKS:\n",
    "   -----------\n",
    "   from langchain_core.callbacks import BaseCallbackHandler\n",
    "   \n",
    "   class MyCallback(BaseCallbackHandler):\n",
    "       def on_llm_new_token(self, token, **kwargs):\n",
    "           print(token, end=\"\")\n",
    "   \n",
    "   model.invoke(messages, config={\"callbacks\": [MyCallback()]})\n",
    "\n",
    "6. BEST PRACTICES:\n",
    "   ----------------\n",
    "   - Use streaming for better UX\n",
    "   - Use async in web applications\n",
    "   - Use concurrent calls for multiple independent requests\n",
    "   - Use callbacks for monitoring/logging\n",
    "   - Handle timeouts and errors gracefully\n",
    "\n",
    "Common Imports:\n",
    "---------------\n",
    "import asyncio\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "=====================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Streaming & Async Module Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Key Patterns:\n",
    "-------------\n",
    "1. .stream() / .astream() for real-time output\n",
    "2. .ainvoke() / .abatch() for async operations\n",
    "3. asyncio.gather() for concurrent requests\n",
    "4. Callbacks for monitoring and logging\n",
    "5. FastAPI + SSE for web streaming\n",
    "\n",
    "Next: 9-RAG.ipynb - Retrieval Augmented Generation\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
