{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LCEL - LangChain Expression Language (LangChain 1.0+)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "=====================================================================\n",
    "LangChain Expression Language (LCEL) - The Core of LangChain\n",
    "=====================================================================\n",
    "\n",
    "LCEL is a declarative way to compose chains using the pipe operator (|).\n",
    "It's the foundation for building complex LLM applications in LangChain.\n",
    "\n",
    "Why LCEL?\n",
    "---------\n",
    "1. Declarative syntax - Easy to read and understand\n",
    "2. Streaming support - Built-in, works automatically\n",
    "3. Async support - Same code works sync and async\n",
    "4. Batch processing - Process multiple inputs efficiently\n",
    "5. Parallel execution - Run independent tasks concurrently\n",
    "6. Retries & fallbacks - Built-in error handling\n",
    "\n",
    "Core Concept: Runnables\n",
    "-----------------------\n",
    "Everything in LCEL is a Runnable - an object with these methods:\n",
    "- .invoke(input)     - Process single input\n",
    "- .batch([inputs])   - Process multiple inputs\n",
    "- .stream(input)     - Stream output tokens\n",
    "- .ainvoke(input)    - Async version of invoke\n",
    "- .abatch([inputs])  - Async version of batch\n",
    "- .astream(input)    - Async version of stream\n",
    "\n",
    "The Pipe Operator (|)\n",
    "---------------------\n",
    "    prompt | model | parser\n",
    "    \n",
    "    Input ‚Üí [Prompt] ‚Üí [Model] ‚Üí [Parser] ‚Üí Output\n",
    "    \n",
    "Each component transforms input and passes output to the next.\n",
    "\n",
    "Updated for LangChain 1.0+ (2025-2026)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "print(\"‚úÖ Environment configured for LCEL examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde858d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Basic LCEL Chain: Prompt | Model | Parser\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "The Classic LCEL Pattern\n",
    "------------------------\n",
    "Most LLM applications follow this pattern:\n",
    "\n",
    "1. Prompt Template - Format user input into a prompt\n",
    "2. Model - Send prompt to LLM, get response\n",
    "3. Output Parser - Extract/format the response\n",
    "\"\"\"\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize model\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0.7)\n",
    "\n",
    "# Create prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that explains concepts simply.\"),\n",
    "    (\"human\", \"Explain {topic} in {num_sentences} sentences.\")\n",
    "])\n",
    "\n",
    "# Create output parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Build chain using LCEL pipe operator\n",
    "chain = prompt | model | parser\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Basic LCEL Chain: prompt | model | parser\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Invoke the chain\n",
    "result = chain.invoke({\n",
    "    \"topic\": \"machine learning\",\n",
    "    \"num_sentences\": 3\n",
    "})\n",
    "\n",
    "print(f\"\\nüìù Result:\\n{result}\")\n",
    "print(f\"\\nüìä Result type: {type(result).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RunnablePassthrough - Passing Data Through\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "RunnablePassthrough\n",
    "-------------------\n",
    "Passes input through unchanged. Useful for:\n",
    "- Preserving original input alongside transformations\n",
    "- Building complex data flows\n",
    "\n",
    "RunnablePassthrough.assign()\n",
    "----------------------------\n",
    "Adds new keys to the input dict while preserving existing ones.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RunnablePassthrough Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example 1: Simple passthrough\n",
    "passthrough = RunnablePassthrough()\n",
    "result = passthrough.invoke({\"name\": \"Alice\", \"age\": 30})\n",
    "print(f\"\\n1Ô∏è‚É£ Simple passthrough:\")\n",
    "print(f\"   Input: {{'name': 'Alice', 'age': 30}}\")\n",
    "print(f\"   Output: {result}\")\n",
    "\n",
    "# Example 2: Passthrough with assign - add new keys\n",
    "passthrough_with_assign = RunnablePassthrough.assign(\n",
    "    greeting=lambda x: f\"Hello, {x['name']}!\",\n",
    "    is_adult=lambda x: x['age'] >= 18\n",
    ")\n",
    "\n",
    "result = passthrough_with_assign.invoke({\"name\": \"Alice\", \"age\": 30})\n",
    "print(f\"\\n2Ô∏è‚É£ Passthrough with assign:\")\n",
    "print(f\"   Input: {{'name': 'Alice', 'age': 30}}\")\n",
    "print(f\"   Output: {result}\")\n",
    "\n",
    "# Example 3: Using passthrough in a chain\n",
    "# This pattern is common in RAG - pass through the question while adding context\n",
    "def get_context(query):\n",
    "    return f\"Context for '{query}': Python is a programming language.\"\n",
    "\n",
    "rag_chain = RunnablePassthrough.assign(\n",
    "    context=lambda x: get_context(x[\"question\"])\n",
    ") | prompt | model | parser\n",
    "\n",
    "# Note: This would need a prompt that accepts both question and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RunnableParallel - Running Tasks Concurrently\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "RunnableParallel\n",
    "----------------\n",
    "Runs multiple runnables in parallel and combines their outputs.\n",
    "Great for:\n",
    "- Generating multiple perspectives\n",
    "- Fetching data from multiple sources\n",
    "- Running independent LLM calls concurrently\n",
    "\n",
    "Syntax:\n",
    "-------\n",
    "RunnableParallel({\"key1\": runnable1, \"key2\": runnable2})\n",
    "# or\n",
    "{\"key1\": runnable1, \"key2\": runnable2}  # Dict is auto-converted\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RunnableParallel Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create different prompts for different perspectives\n",
    "positive_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Focus only on POSITIVE aspects. Be enthusiastic.\"),\n",
    "    (\"human\", \"What do you think about {topic}?\")\n",
    "])\n",
    "\n",
    "negative_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Focus only on CHALLENGES and concerns. Be critical.\"),\n",
    "    (\"human\", \"What do you think about {topic}?\")\n",
    "])\n",
    "\n",
    "neutral_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Provide a balanced, factual analysis.\"),\n",
    "    (\"human\", \"What do you think about {topic}?\")\n",
    "])\n",
    "\n",
    "# Create parallel chain - all three run concurrently!\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"positive\": positive_prompt | model | parser,\n",
    "    \"negative\": negative_prompt | model | parser,\n",
    "    \"neutral\": neutral_prompt | model | parser,\n",
    "})\n",
    "\n",
    "# Run all perspectives in parallel\n",
    "print(\"\\nüîÑ Running 3 LLM calls in parallel...\")\n",
    "results = parallel_chain.invoke({\"topic\": \"artificial intelligence\"})\n",
    "\n",
    "print(\"\\n‚úÖ Positive View:\")\n",
    "print(f\"   {results['positive'][:150]}...\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Concerns:\")\n",
    "print(f\"   {results['negative'][:150]}...\")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è Balanced View:\")\n",
    "print(f\"   {results['neutral'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RunnableLambda - Custom Functions in Chains\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "RunnableLambda\n",
    "--------------\n",
    "Wraps any Python function as a Runnable.\n",
    "Allows custom logic in your chains.\n",
    "\n",
    "Use cases:\n",
    "- Data transformation\n",
    "- Validation\n",
    "- Logging\n",
    "- Calling external APIs\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RunnableLambda Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example 1: Simple transformation\n",
    "def uppercase(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "uppercase_runnable = RunnableLambda(uppercase)\n",
    "print(f\"\\n1Ô∏è‚É£ Simple function:\")\n",
    "print(f\"   Input: 'hello world'\")\n",
    "print(f\"   Output: {uppercase_runnable.invoke('hello world')}\")\n",
    "\n",
    "# Example 2: Complex transformation\n",
    "def extract_and_format(data: dict) -> dict:\n",
    "    \"\"\"Custom processing function.\"\"\"\n",
    "    return {\n",
    "        \"formatted_name\": data[\"name\"].title(),\n",
    "        \"year_born\": 2024 - data[\"age\"],\n",
    "        \"original\": data\n",
    "    }\n",
    "\n",
    "transform = RunnableLambda(extract_and_format)\n",
    "print(f\"\\n2Ô∏è‚É£ Complex transformation:\")\n",
    "result = transform.invoke({\"name\": \"john doe\", \"age\": 30})\n",
    "print(f\"   Output: {result}\")\n",
    "\n",
    "# Example 3: Using lambda in a chain\n",
    "chain_with_lambda = (\n",
    "    RunnableLambda(lambda x: {\"topic\": x[\"topic\"].lower(), \"num_sentences\": 2})\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    "    | RunnableLambda(lambda x: f\"üìö {x}\")\n",
    ")\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£ Lambda in chain:\")\n",
    "result = chain_with_lambda.invoke({\"topic\": \"PYTHON\"})\n",
    "print(f\"   {result[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Batch Processing - Multiple Inputs at Once\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    ".batch() Method\n",
    "---------------\n",
    "Process multiple inputs efficiently.\n",
    "- Same chain, multiple inputs\n",
    "- Automatic parallelization\n",
    "- Better throughput than sequential .invoke()\n",
    "\n",
    "Options:\n",
    "- max_concurrency: Limit parallel requests\n",
    "- return_exceptions: Return errors instead of raising\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Batch Processing Example\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simple chain\n",
    "simple_chain = prompt | model | parser\n",
    "\n",
    "# Multiple topics to process\n",
    "topics = [\n",
    "    {\"topic\": \"quantum computing\", \"num_sentences\": 2},\n",
    "    {\"topic\": \"blockchain\", \"num_sentences\": 2},\n",
    "    {\"topic\": \"neural networks\", \"num_sentences\": 2},\n",
    "]\n",
    "\n",
    "print(f\"\\nüîÑ Processing {len(topics)} topics in batch...\")\n",
    "\n",
    "# Batch process with concurrency limit\n",
    "results = simple_chain.batch(\n",
    "    topics,\n",
    "    config={\"max_concurrency\": 3}  # Limit concurrent API calls\n",
    ")\n",
    "\n",
    "for i, (topic, result) in enumerate(zip(topics, results)):\n",
    "    print(f\"\\nüìù Topic {i+1}: {topic['topic']}\")\n",
    "    print(f\"   {result[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Fallbacks - Handling Errors Gracefully\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    ".with_fallbacks() Method\n",
    "------------------------\n",
    "Provides backup chains if the primary fails.\n",
    "\n",
    "Use cases:\n",
    "- Fallback to cheaper model if premium fails\n",
    "- Fallback to different provider\n",
    "- Graceful degradation\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Fallback Example\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate a failing chain\n",
    "def sometimes_fails(x):\n",
    "    import random\n",
    "    if random.random() < 0.5:\n",
    "        raise Exception(\"Random failure!\")\n",
    "    return f\"Success: {x}\"\n",
    "\n",
    "def always_works(x):\n",
    "    return f\"Fallback result: {x}\"\n",
    "\n",
    "# Primary chain that might fail\n",
    "primary = RunnableLambda(sometimes_fails)\n",
    "\n",
    "# Fallback chain that always works\n",
    "fallback = RunnableLambda(always_works)\n",
    "\n",
    "# Chain with fallback\n",
    "robust_chain = primary.with_fallbacks([fallback])\n",
    "\n",
    "print(\"\\nüîÑ Running chain with fallback 5 times:\")\n",
    "for i in range(5):\n",
    "    result = robust_chain.invoke(f\"test_{i}\")\n",
    "    print(f\"   Run {i+1}: {result}\")\n",
    "\n",
    "# Real-world example: Model fallback\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Model Fallback Pattern\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "# Primary: Use GPT-4o\n",
    "primary_model = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "\n",
    "# Fallback: Use GPT-4o-mini (cheaper, faster)\n",
    "fallback_model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "# Chain with fallback\n",
    "robust_model = primary_model.with_fallbacks([fallback_model])\n",
    "\n",
    "# If GPT-4o fails (rate limit, error), automatically tries GPT-4o-mini\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RunnableBranch - Conditional Logic\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "RunnableBranch\n",
    "--------------\n",
    "Routes to different chains based on conditions.\n",
    "Like an if-elif-else for chains.\n",
    "\n",
    "Structure:\n",
    "RunnableBranch(\n",
    "    (condition1, chain1),\n",
    "    (condition2, chain2),\n",
    "    default_chain\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RunnableBranch - Conditional Routing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Different prompts for different question types\n",
    "math_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a math tutor. Show step-by-step solutions.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "code_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a coding expert. Provide code examples.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "general_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Create chains\n",
    "math_chain = math_prompt | model | parser\n",
    "code_chain = code_prompt | model | parser\n",
    "general_chain = general_prompt | model | parser\n",
    "\n",
    "# Route based on question content\n",
    "def is_math_question(x):\n",
    "    keywords = [\"calculate\", \"solve\", \"math\", \"equation\", \"sum\", \"+\", \"-\", \"*\", \"/\"]\n",
    "    return any(kw in x[\"question\"].lower() for kw in keywords)\n",
    "\n",
    "def is_code_question(x):\n",
    "    keywords = [\"code\", \"python\", \"javascript\", \"function\", \"program\", \"debug\"]\n",
    "    return any(kw in x[\"question\"].lower() for kw in keywords)\n",
    "\n",
    "# Create branching logic\n",
    "router = RunnableBranch(\n",
    "    (is_math_question, math_chain),\n",
    "    (is_code_question, code_chain),\n",
    "    general_chain  # Default\n",
    ")\n",
    "\n",
    "# Test different question types\n",
    "questions = [\n",
    "    {\"question\": \"Calculate 15% of 200\"},\n",
    "    {\"question\": \"Write a Python function to reverse a string\"},\n",
    "    {\"question\": \"What is the capital of France?\"},\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    result = router.invoke(q)\n",
    "    route = \"MATH\" if is_math_question(q) else \"CODE\" if is_code_question(q) else \"GENERAL\"\n",
    "    print(f\"\\n‚ùì Question: {q['question']}\")\n",
    "    print(f\"üîÄ Route: {route}\")\n",
    "    print(f\"üí¨ Answer: {result[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Summary: LCEL in LangChain 1.0+\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "=====================================================================\n",
    "KEY TAKEAWAYS - LangChain Expression Language\n",
    "=====================================================================\n",
    "\n",
    "1. PIPE OPERATOR (|):\n",
    "   ------------------\n",
    "   chain = prompt | model | parser\n",
    "   result = chain.invoke({\"input\": \"...\"})\n",
    "\n",
    "2. CORE RUNNABLES:\n",
    "   ----------------\n",
    "   - RunnablePassthrough - Pass data through unchanged\n",
    "   - RunnablePassthrough.assign() - Add new keys\n",
    "   - RunnableParallel - Run multiple chains concurrently\n",
    "   - RunnableLambda - Wrap any function\n",
    "   - RunnableBranch - Conditional routing\n",
    "\n",
    "3. EXECUTION METHODS:\n",
    "   ------------------\n",
    "   chain.invoke(input)       # Single input\n",
    "   chain.batch([inputs])     # Multiple inputs\n",
    "   chain.stream(input)       # Stream output\n",
    "   chain.ainvoke(input)      # Async single\n",
    "   chain.abatch([inputs])    # Async batch\n",
    "   chain.astream(input)      # Async stream\n",
    "\n",
    "4. ERROR HANDLING:\n",
    "   ----------------\n",
    "   chain.with_fallbacks([backup_chain])\n",
    "   chain.with_retry(stop_after_attempt=3)\n",
    "\n",
    "5. COMMON PATTERNS:\n",
    "   -----------------\n",
    "   # Basic chain\n",
    "   prompt | model | parser\n",
    "   \n",
    "   # Parallel processing\n",
    "   RunnableParallel({\"a\": chain1, \"b\": chain2})\n",
    "   \n",
    "   # Add context\n",
    "   RunnablePassthrough.assign(context=get_context) | prompt | model\n",
    "   \n",
    "   # Conditional routing\n",
    "   RunnableBranch((condition, chain), default)\n",
    "\n",
    "Common Imports:\n",
    "---------------\n",
    "from langchain_core.runnables import (\n",
    "    RunnablePassthrough,\n",
    "    RunnableParallel,\n",
    "    RunnableLambda,\n",
    "    RunnableBranch,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "=====================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LCEL Module Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "LCEL is the foundation of LangChain. Master these patterns:\n",
    "\n",
    "1. prompt | model | parser (basic chain)\n",
    "2. RunnableParallel for concurrent processing\n",
    "3. RunnablePassthrough.assign() for adding data\n",
    "4. RunnableLambda for custom functions\n",
    "5. RunnableBranch for routing logic\n",
    "6. .with_fallbacks() for error handling\n",
    "\n",
    "Next: 8-streaming.ipynb - Streaming and async patterns\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
