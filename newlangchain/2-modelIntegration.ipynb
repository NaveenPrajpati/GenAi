{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52b224",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Model Integration - Multi-Provider Support (LangChain 1.0+)\n# =============================================================================\n\"\"\"\n=====================================================================\nModel Integration - Working with Multiple LLM Providers\n=====================================================================\n\nLangChain 1.0+ provides a unified interface to work with different\nLLM providers through init_chat_model() - a universal model initializer.\n\nSupported Providers:\n--------------------\n- OpenAI (GPT-4, GPT-4o, GPT-4.1, etc.)\n- Anthropic (Claude 3, Claude 3.5, Claude 4)\n- Google (Gemini Pro, Gemini Ultra)\n- Groq (Llama, Mixtral, Qwen with ultra-fast inference)\n- Cohere, Mistral, and more...\n\nKey Concepts:\n-------------\n1. init_chat_model() - Universal model initialization\n2. Streaming responses\n3. Provider-specific features\n4. Model switching without code changes\n\nArchitecture:\n-------------\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚         init_chat_model()           â”‚\n    â”‚     (Universal Interface)           â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                    â”‚\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚           â”‚           â”‚\n        â–¼           â–¼           â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”\n    â”‚OpenAI â”‚   â”‚ Groq  â”‚   â”‚Google â”‚\n    â”‚GPT-4o â”‚   â”‚ Qwen  â”‚   â”‚Gemini â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜\n\nUpdated for LangChain 1.0+ (2025-2026)\n\"\"\"\n\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Set up multiple provider API keys\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\nos.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\nos.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n\nprint(\"âœ… API keys configured for multiple providers\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde858d0",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Method 1: init_chat_model() - Universal Model Initialization\n# =============================================================================\n\"\"\"\ninit_chat_model() automatically detects the provider from the model name\nand initializes the correct chat model class.\n\nSupported model name patterns:\n- OpenAI: \"gpt-4o\", \"gpt-4o-mini\", \"gpt-4.1\", \"gpt-4.1-mini\"\n- Anthropic: \"claude-3-opus\", \"claude-3-sonnet\", \"claude-3-haiku\"\n- Google: \"gemini-pro\", \"gemini-1.5-pro\"\n- Groq: Requires explicit ChatGroq class\n\nBenefits:\n- No need to import provider-specific classes\n- Consistent API across all providers\n- Easy model switching\n\"\"\"\n\nfrom langchain.chat_models import init_chat_model\nfrom langchain_groq import ChatGroq\n\n# =============================================================================\n# OpenAI Models via init_chat_model()\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Example 1: OpenAI with init_chat_model()\")\nprint(\"=\" * 60)\n\n# Simple initialization - auto-detects OpenAI\nopenai_model = init_chat_model(\"gpt-4o-mini\")\n\n# Invoke with a simple question\nresponse = openai_model.invoke(\"What is the capital of France?\")\nprint(f\"\\nâœ… OpenAI Response: {response.content}\")\nprint(f\"ğŸ“Š Model Used: {response.response_metadata.get('model_name', 'unknown')}\")\nprint(f\"ğŸ“ˆ Tokens: {response.response_metadata.get('token_usage', {})}\")\n\n# =============================================================================\n# Groq Models (Ultra-Fast Inference)\n# =============================================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Example 2: Groq (Fast Inference with Reasoning)\")\nprint(\"=\" * 60)\n\n# Groq supports reasoning models like Qwen\ngroq_model = ChatGroq(\n    model=\"qwen/qwen3-32b\",      # Qwen 3 32B with reasoning\n    temperature=0,               # Deterministic output\n    max_tokens=None,             # No limit\n    reasoning_format=\"parsed\",   # Parse reasoning separately\n    timeout=None,\n    max_retries=2,\n)\n\nprint(\"\\nğŸš€ Groq is known for extremely fast inference speeds!\")\nprint(\"   Supports: Llama, Mixtral, Qwen, and more\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2595a67",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Streaming Responses\n# =============================================================================\n\"\"\"\nStreaming provides real-time output as the model generates text.\nUseful for:\n- Better user experience (see output immediately)\n- Long responses\n- Chatbots and interactive applications\n\nMethods:\n- .stream() - Yields chunks as they arrive\n- .astream() - Async streaming\n\"\"\"\n\nprint(\"=\" * 60)\nprint(\"Example 3: Streaming with Groq\")\nprint(\"=\" * 60)\nprint(\"\\nğŸ“ Streaming response (each | is a new chunk):\\n\")\n\n# Stream the response\nfor chunk in groq_model.stream(\"What is the capital of France? Tell me a fun fact about it.\"):\n    # chunk.text contains the content (may be empty for reasoning chunks)\n    if hasattr(chunk, 'text') and chunk.text:\n        print(chunk.text, end=\"|\", flush=True)\n    elif hasattr(chunk, 'content') and chunk.content:\n        print(chunk.content, end=\"|\", flush=True)\n\nprint(\"\\n\\nâœ… Streaming complete!\")\n\n# =============================================================================\n# Comparing Streaming Output Formats\n# =============================================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Example 4: OpenAI Streaming\")\nprint(\"=\" * 60)\nprint(\"\\nğŸ“ OpenAI streaming:\\n\")\n\nfor chunk in openai_model.stream(\"Name 3 programming languages\"):\n    if chunk.content:\n        print(chunk.content, end=\"\", flush=True)\n\nprint(\"\\n\")"
  },
  {
   "cell_type": "code",
   "id": "aswh9rhi8lb",
   "source": "# =============================================================================\n# Summary: Model Integration Key Points\n# =============================================================================\n\"\"\"\nModel Integration - Quick Reference:\n------------------------------------\n\nâœ… UNIVERSAL INITIALIZATION:\n   from langchain.chat_models import init_chat_model\n   model = init_chat_model(\"gpt-4o-mini\")  # Auto-detects provider\n\nâœ… PROVIDER-SPECIFIC CLASSES:\n   from langchain_openai import ChatOpenAI\n   from langchain_groq import ChatGroq\n   from langchain_google_genai import ChatGoogleGenerativeAI\n   from langchain_anthropic import ChatAnthropic\n\nâœ… COMMON PARAMETERS:\n   - model: Model name/ID\n   - temperature: 0-2 (0 = deterministic, higher = creative)\n   - max_tokens: Maximum output length\n   - timeout: Request timeout\n   - max_retries: Retry attempts\n\nâœ… INVOCATION METHODS:\n   model.invoke(\"question\")          # Single call\n   model.stream(\"question\")          # Streaming\n   model.batch([\"q1\", \"q2\"])         # Multiple inputs\n   await model.ainvoke(\"question\")   # Async\n\nâœ… RESPONSE STRUCTURE:\n   response.content              # The text response\n   response.response_metadata    # Model info, tokens, etc.\n   response.usage_metadata       # Token counts\n\nPopular Models (2025-2026):\n---------------------------\n- OpenAI: gpt-4o, gpt-4o-mini, gpt-4.1, gpt-4.1-mini\n- Anthropic: claude-3-opus, claude-3-sonnet, claude-3-haiku\n- Google: gemini-1.5-pro, gemini-1.5-flash\n- Groq: llama-3-70b, mixtral-8x7b, qwen/qwen3-32b\n\nPro Tips:\n---------\n- Use init_chat_model() for quick prototyping\n- Use provider-specific classes for advanced features\n- Set temperature=0 for deterministic outputs\n- Use streaming for better UX in chat applications\n\"\"\"\n\nprint(\"=\" * 60)\nprint(\"ğŸ“š Summary: Model Integration\")\nprint(\"=\" * 60)\nprint(\"\"\"\nKey Methods:\n- init_chat_model(\"model-name\")  â†’ Universal initialization\n- model.invoke(\"prompt\")         â†’ Single call\n- model.stream(\"prompt\")         â†’ Streaming output\n- model.batch([...])             â†’ Multiple inputs\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}