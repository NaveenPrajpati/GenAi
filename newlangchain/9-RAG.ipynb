{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RAG - Retrieval Augmented Generation (LangChain 1.0+)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "=====================================================================\n",
    "Retrieval Augmented Generation (RAG)\n",
    "=====================================================================\n",
    "\n",
    "What is RAG?\n",
    "------------\n",
    "RAG enhances LLM responses by retrieving relevant documents\n",
    "and including them as context. This allows LLMs to:\n",
    "- Answer questions about YOUR data\n",
    "- Stay up-to-date (no training cutoff limitation)\n",
    "- Reduce hallucinations with grounded facts\n",
    "\n",
    "RAG Pipeline:\n",
    "-------------\n",
    "    User Query\n",
    "         ‚îÇ\n",
    "         ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ   1. EMBED      ‚îÇ  Convert query to vector\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "         ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  2. RETRIEVE    ‚îÇ  Find similar documents\n",
    "    ‚îÇ  (Vector Store) ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "         ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  3. AUGMENT     ‚îÇ  Add docs to prompt\n",
    "    ‚îÇ  (Prompt)       ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "         ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  4. GENERATE    ‚îÇ  LLM creates response\n",
    "    ‚îÇ  (Model)        ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "         ‚ñº\n",
    "    Final Answer\n",
    "\n",
    "Updated for LangChain 1.0+ (2025-2026)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "print(\"‚úÖ Environment configured for RAG examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde858d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 1: Document Loading\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Document Loaders\n",
    "----------------\n",
    "Load data from various sources:\n",
    "- TextLoader - Plain text files\n",
    "- PDFLoader - PDF documents\n",
    "- WebBaseLoader - Web pages\n",
    "- CSVLoader - CSV files\n",
    "- DirectoryLoader - Multiple files from a folder\n",
    "\n",
    "Each loader returns Document objects with:\n",
    "- page_content: The text content\n",
    "- metadata: Source info, page numbers, etc.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 1: Document Loading\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# For this tutorial, we'll create sample documents\n",
    "# In production, you'd use loaders like TextLoader, PDFLoader, etc.\n",
    "\n",
    "sample_documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"LangChain is a framework for developing applications powered by \n",
    "        large language models (LLMs). It provides tools for prompt management, chains, \n",
    "        agents, and memory management. LangChain was created by Harrison Chase in 2022.\"\"\",\n",
    "        metadata={\"source\": \"langchain_intro.txt\", \"topic\": \"overview\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"LCEL (LangChain Expression Language) is a declarative way to compose \n",
    "        chains using the pipe operator. It supports streaming, async, batch processing, \n",
    "        and parallel execution out of the box. LCEL is the recommended way to build chains.\"\"\",\n",
    "        metadata={\"source\": \"lcel_guide.txt\", \"topic\": \"lcel\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"RAG (Retrieval Augmented Generation) combines LLMs with external \n",
    "        knowledge retrieval. It helps reduce hallucinations and allows LLMs to answer \n",
    "        questions about specific documents or data not in their training set.\"\"\",\n",
    "        metadata={\"source\": \"rag_overview.txt\", \"topic\": \"rag\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Vector stores like FAISS, Chroma, and Pinecone store document \n",
    "        embeddings for fast similarity search. They are essential components of RAG \n",
    "        systems, enabling semantic search over large document collections.\"\"\",\n",
    "        metadata={\"source\": \"vector_stores.txt\", \"topic\": \"storage\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"LangChain agents can use tools to interact with external systems. \n",
    "        The create_agent() function in LangChain 1.0 is the standard way to create agents. \n",
    "        Agents can search the web, query databases, or call APIs.\"\"\",\n",
    "        metadata={\"source\": \"agents_guide.txt\", \"topic\": \"agents\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"\\nüìÑ Loaded {len(sample_documents)} documents\")\n",
    "for i, doc in enumerate(sample_documents):\n",
    "    print(f\"   {i+1}. {doc.metadata['source']} ({doc.metadata['topic']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 2: Text Splitting\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Text Splitters\n",
    "--------------\n",
    "Documents often need to be split into smaller chunks:\n",
    "- Fit within context window limits\n",
    "- More precise retrieval\n",
    "- Better embedding quality\n",
    "\n",
    "Common splitters:\n",
    "- RecursiveCharacterTextSplitter - Best general-purpose\n",
    "- CharacterTextSplitter - Simple character-based\n",
    "- TokenTextSplitter - Based on token count\n",
    "\n",
    "Key parameters:\n",
    "- chunk_size: Maximum characters per chunk\n",
    "- chunk_overlap: Characters shared between chunks\n",
    "\"\"\"\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 2: Text Splitting\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,       # Max characters per chunk\n",
    "    chunk_overlap=50,     # Overlap between chunks\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Split priorities\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "splits = text_splitter.split_documents(sample_documents)\n",
    "\n",
    "print(f\"\\nüìÑ Original: {len(sample_documents)} documents\")\n",
    "print(f\"üìã After splitting: {len(splits)} chunks\")\n",
    "print(f\"\\nüìù Sample chunks:\")\n",
    "for i, chunk in enumerate(splits[:3]):\n",
    "    print(f\"\\n   Chunk {i+1} ({len(chunk.page_content)} chars):\")\n",
    "    print(f\"   '{chunk.page_content[:80]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 3: Embeddings\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Embeddings\n",
    "----------\n",
    "Convert text into numerical vectors that capture semantic meaning.\n",
    "Similar texts have similar vectors (close in vector space).\n",
    "\n",
    "Popular embedding models:\n",
    "- OpenAI: text-embedding-3-small, text-embedding-3-large\n",
    "- Hugging Face: all-MiniLM-L6-v2, BGE models\n",
    "- Cohere: embed-english-v3.0\n",
    "- Google: textembedding-gecko\n",
    "\"\"\"\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 3: Creating Embeddings\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize embeddings model\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"  # Fast and cost-effective\n",
    ")\n",
    "\n",
    "# Test embedding a single text\n",
    "sample_text = \"What is LangChain?\"\n",
    "sample_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "print(f\"\\nüìä Embedding model: text-embedding-3-small\")\n",
    "print(f\"üìè Vector dimension: {len(sample_embedding)}\")\n",
    "print(f\"üìù Sample text: '{sample_text}'\")\n",
    "print(f\"üî¢ First 5 values: {sample_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 4: Vector Store\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Vector Stores\n",
    "-------------\n",
    "Store embeddings for fast similarity search.\n",
    "\n",
    "Options:\n",
    "- FAISS - Fast, in-memory, free (great for development)\n",
    "- Chroma - Persistent, easy to use\n",
    "- Pinecone - Managed cloud service\n",
    "- Weaviate - Full-featured, open source\n",
    "- Qdrant - High performance, filtering support\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 4: Creating Vector Store\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create vector store from documents\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Vector store created with {len(splits)} chunks\")\n",
    "\n",
    "# Test similarity search\n",
    "query = \"What is LCEL?\"\n",
    "similar_docs = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "print(f\"\\nüîç Query: '{query}'\")\n",
    "print(f\"üìÑ Top 2 similar documents:\")\n",
    "for i, doc in enumerate(similar_docs):\n",
    "    print(f\"\\n   {i+1}. {doc.page_content[:100]}...\")\n",
    "    print(f\"      Source: {doc.metadata.get('source', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 5: Retriever\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Retrievers\n",
    "----------\n",
    "Wrap vector stores with a consistent interface.\n",
    "Retrievers are Runnables - they work with LCEL!\n",
    "\n",
    "Retriever Types:\n",
    "- VectorStoreRetriever - Basic similarity search\n",
    "- MultiQueryRetriever - Generates multiple query variants\n",
    "- ContextualCompressionRetriever - Compresses retrieved docs\n",
    "- EnsembleRetriever - Combines multiple retrievers\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 5: Creating Retriever\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create retriever from vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  # or \"mmr\" for diversity\n",
    "    search_kwargs={\"k\": 3}     # Return top 3 documents\n",
    ")\n",
    "\n",
    "# Test retriever (it's a Runnable!)\n",
    "query = \"How do agents work in LangChain?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"\\nüîç Query: '{query}'\")\n",
    "print(f\"üìÑ Retrieved {len(retrieved_docs)} documents:\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n   {i+1}. {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 6: Complete RAG Chain\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Building the RAG Chain\n",
    "----------------------\n",
    "Combine all components using LCEL:\n",
    "\n",
    "1. Retriever gets relevant documents\n",
    "2. Prompt formats docs + question\n",
    "3. Model generates answer\n",
    "4. Parser extracts text\n",
    "\"\"\"\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 6: Complete RAG Chain\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize model\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0)\n",
    "\n",
    "# RAG prompt template\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful assistant that answers questions based on the \n",
    "provided context. If the answer is not in the context, say so.\n",
    "\n",
    "Context:\n",
    "{context}\"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build RAG chain\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Retrieve and format\n",
    "        \"question\": RunnablePassthrough()     # Pass question through\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ RAG chain built!\")\n",
    "print(\"\\nüìä Chain structure:\")\n",
    "print(\"   retriever ‚Üí format_docs ‚Üí prompt ‚Üí model ‚Üí parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 7: Using the RAG Chain\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Testing Our RAG System\n",
    "----------------------\n",
    "Ask questions that require knowledge from our documents.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 7: Testing RAG Chain\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test questions\n",
    "questions = [\n",
    "    \"What is LCEL and what are its benefits?\",\n",
    "    \"Who created LangChain and when?\",\n",
    "    \"What are vector stores used for in RAG?\",\n",
    "    \"What is quantum computing?\"  # Not in our docs!\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    answer = rag_chain.invoke(question)\n",
    "    print(f\"üí¨ Answer: {answer}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Advanced: RAG with Sources\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "RAG with Source Attribution\n",
    "---------------------------\n",
    "Return both the answer AND the source documents.\n",
    "This helps users verify the information.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Advanced: RAG with Sources\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# RAG chain that returns sources too\n",
    "rag_chain_with_sources = RunnableParallel({\n",
    "    \"answer\": rag_chain,\n",
    "    \"sources\": retriever  # Also return the source documents\n",
    "})\n",
    "\n",
    "# Test with sources\n",
    "question = \"What is RAG and why is it useful?\"\n",
    "result = rag_chain_with_sources.invoke(question)\n",
    "\n",
    "print(f\"\\n‚ùì Question: {question}\")\n",
    "print(f\"\\nüí¨ Answer: {result['answer']}\")\n",
    "print(f\"\\nüìö Sources:\")\n",
    "for i, doc in enumerate(result['sources']):\n",
    "    source = doc.metadata.get('source', 'unknown')\n",
    "    print(f\"   {i+1}. {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Advanced: Conversational RAG\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Conversational RAG\n",
    "------------------\n",
    "Allow follow-up questions that reference previous context.\n",
    "Uses chat history to reformulate queries.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Advanced: Conversational RAG\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prompt to reformulate questions with context\n",
    "contextualize_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Given the chat history and latest question, \n",
    "reformulate the question to be standalone (understandable without history).\n",
    "If the question is already standalone, return it as-is.\"\"\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Chain to reformulate question\n",
    "contextualize_chain = contextualize_prompt | model | StrOutputParser()\n",
    "\n",
    "# Full conversational RAG\n",
    "def conversational_rag(question: str, chat_history: list):\n",
    "    # If there's history, reformulate the question\n",
    "    if chat_history:\n",
    "        standalone_question = contextualize_chain.invoke({\n",
    "            \"chat_history\": chat_history,\n",
    "            \"question\": question\n",
    "        })\n",
    "    else:\n",
    "        standalone_question = question\n",
    "    \n",
    "    # Get answer using RAG\n",
    "    answer = rag_chain.invoke(standalone_question)\n",
    "    return answer, standalone_question\n",
    "\n",
    "# Simulate a conversation\n",
    "chat_history = []\n",
    "conversation = [\n",
    "    \"What is LangChain?\",\n",
    "    \"Who created it?\",  # References LangChain from previous turn\n",
    "    \"What year?\"        # References creator from previous turn\n",
    "]\n",
    "\n",
    "for question in conversation:\n",
    "    answer, reformulated = conversational_rag(question, chat_history)\n",
    "    \n",
    "    print(f\"\\nüë§ User: {question}\")\n",
    "    if reformulated != question:\n",
    "        print(f\"   (Reformulated: {reformulated})\")\n",
    "    print(f\"ü§ñ Assistant: {answer}\")\n",
    "    \n",
    "    # Update history\n",
    "    chat_history.extend([\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=answer)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Summary: RAG in LangChain 1.0+\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "=====================================================================\n",
    "KEY TAKEAWAYS - RAG Pipeline\n",
    "=====================================================================\n",
    "\n",
    "1. LOAD DOCUMENTS:\n",
    "   -----------------\n",
    "   from langchain_community.document_loaders import TextLoader\n",
    "   docs = TextLoader(\"file.txt\").load()\n",
    "\n",
    "2. SPLIT TEXT:\n",
    "   ------------\n",
    "   from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "   splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "   chunks = splitter.split_documents(docs)\n",
    "\n",
    "3. CREATE EMBEDDINGS:\n",
    "   -------------------\n",
    "   from langchain_openai import OpenAIEmbeddings\n",
    "   embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "4. CREATE VECTOR STORE:\n",
    "   ---------------------\n",
    "   from langchain_community.vectorstores import FAISS\n",
    "   vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "5. CREATE RETRIEVER:\n",
    "   ------------------\n",
    "   retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "6. BUILD RAG CHAIN:\n",
    "   -----------------\n",
    "   rag_chain = (\n",
    "       {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "       | rag_prompt\n",
    "       | model\n",
    "       | StrOutputParser()\n",
    "   )\n",
    "\n",
    "7. USE THE CHAIN:\n",
    "   ---------------\n",
    "   answer = rag_chain.invoke(\"Your question here\")\n",
    "\n",
    "Common Imports:\n",
    "---------------\n",
    "from langchain_community.document_loaders import TextLoader, PDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "=====================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RAG Module Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "RAG Pipeline Summary:\n",
    "---------------------\n",
    "1. Load ‚Üí 2. Split ‚Üí 3. Embed ‚Üí 4. Store ‚Üí 5. Retrieve ‚Üí 6. Generate\n",
    "\n",
    "Key Components:\n",
    "---------------\n",
    "- Document Loaders (TextLoader, PDFLoader, WebBaseLoader)\n",
    "- Text Splitters (RecursiveCharacterTextSplitter)\n",
    "- Embeddings (OpenAIEmbeddings, HuggingFaceEmbeddings)\n",
    "- Vector Stores (FAISS, Chroma, Pinecone)\n",
    "- Retrievers (as_retriever(), MultiQueryRetriever)\n",
    "\n",
    "Next: 10-agents.ipynb - Agent patterns with create_agent()\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
