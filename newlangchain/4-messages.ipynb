{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52b224",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Messages - The Building Blocks of LLM Conversations (LangChain 1.0+)\n# =============================================================================\n\"\"\"\n=====================================================================\nUnderstanding LangChain Messages\n=====================================================================\n\nMessages are the fundamental units of communication with LLMs.\nEvery conversation is represented as a list of messages.\n\nMessage Types:\n--------------\n1. SystemMessage - Sets the AI's behavior/persona (invisible to users)\n2. HumanMessage - User's input/questions\n3. AIMessage - Model's responses\n4. ToolMessage - Results from tool execution\n5. FunctionMessage - (Deprecated) Old way of returning function results\n\nMessage Flow in a Conversation:\n-------------------------------\n    [SystemMessage]     \"You are a helpful assistant\"\n           â”‚\n           â–¼\n    [HumanMessage]      \"What's 2+2?\"\n           â”‚\n           â–¼\n    [AIMessage]         \"2+2 equals 4\"\n           â”‚\n           â–¼\n    [HumanMessage]      \"And 3+3?\"\n           â”‚\n           â–¼\n    [AIMessage]         \"3+3 equals 6\"\n\nWhy Messages Matter:\n--------------------\n- LLMs are stateless - they don't remember previous turns\n- YOU must pass the full conversation history each time\n- Message order matters (system â†’ human â†’ ai â†’ human â†’ ai...)\n- Context window limits how many messages you can include\n\nUpdated for LangChain 1.0+ (2025-2026)\n\"\"\"\n\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\nos.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\nos.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n\nprint(\"âœ… Environment configured for messages examples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde858d0",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Core Message Types (LangChain 1.0+)\n# =============================================================================\n\"\"\"\nThe Four Essential Message Types\n--------------------------------\nImport from langchain_core.messages (NOT langchain.messages)\n\nLangChain 1.0+ uses langchain_core for all core abstractions.\n\"\"\"\n\nfrom langchain_core.messages import (\n    SystemMessage,\n    HumanMessage,\n    AIMessage,\n    ToolMessage,\n)\nfrom langchain.chat_models import init_chat_model\n\n# Initialize model\nmodel = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0.7)\n\n# =============================================================================\n# 1. SystemMessage - Sets AI Behavior\n# =============================================================================\n\"\"\"\nSystemMessage:\n- First message in the conversation (usually)\n- Defines the AI's persona, rules, and behavior\n- Not visible to the user in most UIs\n- Affects ALL subsequent responses\n\"\"\"\n\nsystem_msg = SystemMessage(\n    content=\"\"\"You are a helpful assistant who specializes in French culture.\n    Always include a fun fact about France when relevant.\n    Keep responses concise but informative.\"\"\"\n)\n\nprint(\"=\" * 60)\nprint(\"1. SystemMessage Example\")\nprint(\"=\" * 60)\nprint(f\"Type: {type(system_msg).__name__}\")\nprint(f\"Content: {system_msg.content[:100]}...\")\n\n# =============================================================================\n# 2. HumanMessage - User Input\n# =============================================================================\n\"\"\"\nHumanMessage:\n- Represents what the user says/asks\n- Can include optional 'name' for multi-user scenarios\n- The main input that drives the conversation\n\"\"\"\n\nhuman_msg = HumanMessage(\n    content=\"What is the capital of France?\",\n    name=\"user_naveen\"  # Optional: useful for multi-user chats\n)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"2. HumanMessage Example\")\nprint(\"=\" * 60)\nprint(f\"Type: {type(human_msg).__name__}\")\nprint(f\"Content: {human_msg.content}\")\nprint(f\"Name: {human_msg.name}\")\n\n# =============================================================================\n# 3. AIMessage - Model Response\n# =============================================================================\n\"\"\"\nAIMessage:\n- Represents the model's response\n- Can contain tool_calls if the model wants to use tools\n- Used to build conversation history\n\"\"\"\n\n# Manual AIMessage (for building history)\nai_msg = AIMessage(\n    content=\"The capital of France is Paris! Fun fact: Paris is called 'La Ville LumiÃ¨re' (The City of Light).\"\n)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"3. AIMessage Example\")\nprint(\"=\" * 60)\nprint(f\"Type: {type(ai_msg).__name__}\")\nprint(f\"Content: {ai_msg.content}\")\nprint(f\"Tool calls: {ai_msg.tool_calls}\")\n\n# =============================================================================\n# Complete Conversation Example\n# =============================================================================\n\nmessages = [\n    system_msg,\n    human_msg,\n]\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Complete Conversation\")\nprint(\"=\" * 60)\n\nresponse = model.invoke(messages)\nprint(f\"User: {human_msg.content}\")\nprint(f\"AI: {response.content}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d426ca0b",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 4. ToolMessage - Tool Execution Results\n# =============================================================================\n\"\"\"\nToolMessage:\n- Contains results from tool execution\n- MUST include tool_call_id matching the original call\n- Sent back to the model so it can use the result\n\nTool Message Flow:\n------------------\n[HumanMessage] â†’ [AIMessage with tool_calls] â†’ [ToolMessage with result] â†’ [AIMessage final]\n\"\"\"\n\n# Simulating a tool call scenario\nfrom langchain_core.tools import tool\n\n@tool\ndef get_population(city: str) -> str:\n    \"\"\"Get the population of a city.\"\"\"\n    populations = {\"Paris\": \"2.1 million\", \"London\": \"8.8 million\", \"Tokyo\": \"13.9 million\"}\n    return populations.get(city, \"Unknown\")\n\n# Bind tool to model\nmodel_with_tools = model.bind_tools([get_population])\n\n# Simulate the complete flow\nmessages_with_tools = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What's the population of Paris?\")\n]\n\nprint(\"=\" * 60)\nprint(\"4. ToolMessage Example - Complete Flow\")\nprint(\"=\" * 60)\n\n# Step 1: Get model response (may contain tool calls)\nresponse = model_with_tools.invoke(messages_with_tools)\nmessages_with_tools.append(response)\n\nprint(f\"\\nðŸ“ User: What's the population of Paris?\")\n\nif response.tool_calls:\n    print(f\"\\nðŸ”§ Model wants to use tool:\")\n    for tc in response.tool_calls:\n        print(f\"   Tool: {tc['name']}\")\n        print(f\"   Args: {tc['args']}\")\n        print(f\"   Call ID: {tc['id']}\")\n        \n        # Step 2: Execute tool and create ToolMessage\n        result = get_population.invoke(tc['args'])\n        tool_msg = ToolMessage(\n            content=str(result),\n            tool_call_id=tc['id']  # MUST match!\n        )\n        messages_with_tools.append(tool_msg)\n        \n        print(f\"\\nðŸ“Š Tool Result:\")\n        print(f\"   Content: {tool_msg.content}\")\n        print(f\"   Tool Call ID: {tool_msg.tool_call_id}\")\n    \n    # Step 3: Get final response\n    final_response = model_with_tools.invoke(messages_with_tools)\n    print(f\"\\nâœ… Final Answer: {final_response.content}\")\nelse:\n    print(f\"\\nâœ… Direct Answer: {response.content}\")"
  },
  {
   "cell_type": "code",
   "id": "0ww6zyi4dl5",
   "source": "# =============================================================================\n# Building Multi-Turn Conversations\n# =============================================================================\n\"\"\"\nConversation Memory Pattern\n---------------------------\nSince LLMs are stateless, YOU manage the conversation history.\nEach call must include all relevant previous messages.\n\nKey Concepts:\n- Append new messages to the list after each turn\n- Include both user messages AND AI responses\n- System message usually stays constant\n\"\"\"\n\ndef chat_with_memory():\n    \"\"\"Simple chatbot with conversation memory.\"\"\"\n    \n    conversation = [\n        SystemMessage(content=\"You are a helpful math tutor. Be encouraging and explain step by step.\")\n    ]\n    \n    # Simulated conversation\n    user_inputs = [\n        \"What is 15% of 80?\",\n        \"Can you explain how you calculated that?\",\n        \"Now what's 20% of the same number?\"\n    ]\n    \n    print(\"=\" * 60)\n    print(\"Multi-Turn Conversation Example\")\n    print(\"=\" * 60)\n    \n    for user_input in user_inputs:\n        # Add user message\n        conversation.append(HumanMessage(content=user_input))\n        \n        # Get AI response\n        response = model.invoke(conversation)\n        \n        # Add AI response to history\n        conversation.append(response)\n        \n        print(f\"\\nðŸ‘¤ User: {user_input}\")\n        print(f\"ðŸ¤– AI: {response.content[:200]}...\" if len(response.content) > 200 else f\"ðŸ¤– AI: {response.content}\")\n    \n    print(f\"\\nðŸ“Š Total messages in conversation: {len(conversation)}\")\n    return conversation\n\n# Run the conversation\nconversation_history = chat_with_memory()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "b29igd8k0xk",
   "source": "# =============================================================================\n# Message Placeholders and Prompt Templates\n# =============================================================================\n\"\"\"\nMessagesPlaceholder - Dynamic Message Insertion\n-----------------------------------------------\nUsed in ChatPromptTemplate to insert variable-length message lists.\nPerfect for conversation history in chatbots.\n\nPattern:\nChatPromptTemplate.from_messages([\n    (\"system\", \"...\"),\n    MessagesPlaceholder(\"history\"),  # Insert conversation here\n    (\"human\", \"{input}\")\n])\n\"\"\"\n\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n# Template with placeholder for history\nchat_template = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Be concise.\"),\n    MessagesPlaceholder(\"chat_history\"),  # Dynamic history goes here\n    (\"human\", \"{user_input}\")\n])\n\nprint(\"=\" * 60)\nprint(\"MessagesPlaceholder Example\")\nprint(\"=\" * 60)\n\n# Build conversation history\nhistory = [\n    HumanMessage(content=\"My name is Naveen\"),\n    AIMessage(content=\"Nice to meet you, Naveen!\"),\n    HumanMessage(content=\"I'm learning LangChain\"),\n    AIMessage(content=\"That's great! LangChain is a powerful framework for building LLM applications.\"),\n]\n\n# Format with history and new input\nformatted = chat_template.invoke({\n    \"chat_history\": history,\n    \"user_input\": \"What's my name and what am I learning?\"\n})\n\nprint(f\"\\nðŸ“‹ Formatted messages:\")\nfor i, msg in enumerate(formatted.messages):\n    msg_type = type(msg).__name__\n    content = msg.content[:80] + \"...\" if len(msg.content) > 80 else msg.content\n    print(f\"   {i+1}. [{msg_type}] {content}\")\n\n# Get response\nresponse = model.invoke(formatted)\nprint(f\"\\nâœ… Response: {response.content}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "pok9dd1gz1",
   "source": "# =============================================================================\n# Message Trimming for Long Conversations\n# =============================================================================\n\"\"\"\nManaging Context Window Limits\n------------------------------\nLLMs have token limits. Long conversations must be trimmed.\n\nStrategies:\n1. Keep last N messages (simple)\n2. Keep system + last N (preserve persona)\n3. Summarize old messages (advanced)\n4. Use trim_messages() utility\n\"\"\"\n\nfrom langchain_core.messages import trim_messages\n\n# Simulate a long conversation\nlong_conversation = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n]\n\n# Add many messages\nfor i in range(10):\n    long_conversation.append(HumanMessage(content=f\"Question {i+1}: What is {i+1} + {i+1}?\"))\n    long_conversation.append(AIMessage(content=f\"Answer: {i+1} + {i+1} = {(i+1)*2}\"))\n\nprint(\"=\" * 60)\nprint(\"Message Trimming Example\")\nprint(\"=\" * 60)\nprint(f\"\\nðŸ“Š Original conversation: {len(long_conversation)} messages\")\n\n# Method 1: Simple slicing - keep system + last 6 messages\ntrimmed_simple = [long_conversation[0]] + long_conversation[-6:]\nprint(f\"ðŸ“Š After simple trim: {len(trimmed_simple)} messages\")\n\n# Method 2: Using trim_messages utility (LangChain built-in)\n# This is more sophisticated - it can trim based on token count\ntrimmed_utility = trim_messages(\n    long_conversation,\n    max_tokens=500,  # Approximate token limit\n    strategy=\"last\",  # Keep last messages\n    token_counter=len,  # Simple character count (use tiktoken for accuracy)\n    include_system=True,  # Always keep system message\n    allow_partial=False,  # Don't cut messages in half\n)\n\nprint(f\"ðŸ“Š After trim_messages: {len(trimmed_utility)} messages\")\n\n# Show trimmed conversation\nprint(\"\\nðŸ“‹ Trimmed conversation:\")\nfor i, msg in enumerate(trimmed_simple):\n    msg_type = type(msg).__name__[:6]\n    content = msg.content[:50] + \"...\" if len(msg.content) > 50 else msg.content\n    print(f\"   {i+1}. [{msg_type}] {content}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0e5w33gnwgy",
   "source": "# =============================================================================\n# Summary: Messages in LangChain 1.0+\n# =============================================================================\n\"\"\"\n=====================================================================\nKEY TAKEAWAYS - Messages in LangChain\n=====================================================================\n\n1. MESSAGE TYPES:\n   ---------------\n   from langchain_core.messages import (\n       SystemMessage,   # AI persona/rules\n       HumanMessage,    # User input\n       AIMessage,       # Model response\n       ToolMessage,     # Tool execution result\n   )\n\n2. BASIC CONVERSATION:\n   --------------------\n   messages = [\n       SystemMessage(content=\"You are helpful.\"),\n       HumanMessage(content=\"Hello!\"),\n   ]\n   response = model.invoke(messages)\n\n3. MULTI-TURN PATTERN:\n   --------------------\n   conversation = [SystemMessage(...)]\n   \n   while True:\n       user_input = get_input()\n       conversation.append(HumanMessage(content=user_input))\n       response = model.invoke(conversation)\n       conversation.append(response)\n\n4. TOOL MESSAGE FLOW:\n   -------------------\n   [Human] â†’ [AI with tool_calls] â†’ [Tool result] â†’ [AI final answer]\n   \n   ToolMessage(\n       content=\"result\",\n       tool_call_id=\"call_abc123\"  # MUST match AIMessage.tool_calls[].id\n   )\n\n5. MESSAGES PLACEHOLDER:\n   ----------------------\n   from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n   \n   template = ChatPromptTemplate.from_messages([\n       (\"system\", \"...\"),\n       MessagesPlaceholder(\"history\"),\n       (\"human\", \"{input}\")\n   ])\n\n6. TRIMMING FOR CONTEXT LIMITS:\n   -----------------------------\n   from langchain_core.messages import trim_messages\n   \n   trimmed = trim_messages(\n       messages,\n       max_tokens=4000,\n       strategy=\"last\",\n       include_system=True\n   )\n\nCommon Imports:\n---------------\nfrom langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.messages import trim_messages\n\n=====================================================================\n\"\"\"\n\nprint(\"=\" * 60)\nprint(\"Messages Module Complete!\")\nprint(\"=\" * 60)\nprint(\"\"\"\nNext Steps:\n-----------\n1. 5-structureoutput.ipynb - Getting structured data from LLMs\n2. 6-middleware.ipynb - Request/response middleware\n\nKey Points:\n-----------\n- Messages are the ONLY way to communicate with LLMs\n- YOU manage conversation history (LLMs are stateless)\n- Always match tool_call_id in ToolMessages\n- Trim messages when approaching context limits\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}