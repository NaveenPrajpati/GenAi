{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba52b224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment configured for tools examples\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Tools - Giving LLMs the Ability to Take Actions (LangChain 1.0+)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "=====================================================================\n",
    "Tools - Extending LLM Capabilities\n",
    "=====================================================================\n",
    "\n",
    "Tools allow LLMs to interact with external systems, APIs, and data.\n",
    "They transform LLMs from text generators into action-taking agents.\n",
    "\n",
    "Key Concepts:\n",
    "-------------\n",
    "1. @tool decorator - Simplest way to create tools\n",
    "2. bind_tools() - Attach tools to a model\n",
    "3. Tool execution - Handle tool calls and responses\n",
    "4. Tool schemas - Automatic schema generation from type hints\n",
    "\n",
    "Tool Calling Flow:\n",
    "------------------\n",
    "    User Question\n",
    "         â”‚\n",
    "         â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚  LLM with Tools â”‚\n",
    "    â”‚  (bind_tools)   â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼ (LLM decides to use tool)\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚   Tool Call     â”‚  {\"name\": \"get_weather\", \"args\": {\"location\": \"NYC\"}}\n",
    "    â”‚   (AIMessage)   â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼ (Execute tool)\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚  Tool Result    â”‚  \"The weather in NYC is sunny\"\n",
    "    â”‚  (ToolMessage)  â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚  Final Answer   â”‚  LLM uses tool result to respond\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Updated for LangChain 1.0+ (2025-2026)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "print(\"âœ… Environment configured for tools examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde858d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naveen/Desktop/web/genAi/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Tool Schema (auto-generated from type hints):\n",
      "============================================================\n",
      "Name: get_weather\n",
      "Description: Get the current weather for a given location.\n",
      "\n",
      "Args:\n",
      "    location: The city or location to get weather for (e.g., \"New York\", \"London\")\n",
      "\n",
      "Returns:\n",
      "    A string describing the current weather conditions\n",
      "Args Schema: {'description': 'Get the current weather for a given location.\\n\\nArgs:\\n    location: The city or location to get weather for (e.g., \"New York\", \"London\")\\n\\nReturns:\\n    A string describing the current weather conditions', 'properties': {'location': {'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'title': 'get_weather', 'type': 'object'}\n",
      "\n",
      "============================================================\n",
      "Tool with Pydantic Schema:\n",
      "============================================================\n",
      "Schema: {'description': 'Input schema for weather tool with validation.', 'properties': {'location': {'description': 'City name or location', 'minLength': 2, 'title': 'Location', 'type': 'string'}, 'units': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': 'fahrenheit', 'description': \"Temperature units: 'fahrenheit' or 'celsius'\", 'title': 'Units'}}, 'required': ['location'], 'title': 'WeatherInput', 'type': 'object'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nb/pbswtk4n7px0p0yxxc0z0lzh0000gn/T/ipykernel_6269/2512067607.py:69: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  print(f\"Args Schema: {get_weather.args_schema.schema() if get_weather.args_schema else 'None'}\")\n",
      "/var/folders/nb/pbswtk4n7px0p0yxxc0z0lzh0000gn/T/ipykernel_6269/2512067607.py:93: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  print(f\"Schema: {get_weather_detailed.args_schema.schema()}\")\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Tool Creation and Binding (LangChain 1.0+)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Three Ways to Create Tools:\n",
    "---------------------------\n",
    "1. @tool decorator - Simplest, uses function signature\n",
    "2. StructuredTool - More control over schema\n",
    "3. BaseTool class - Full customization\n",
    "\n",
    "bind_tools() - The Key Method:\n",
    "------------------------------\n",
    "Attaches tools to a model, enabling the LLM to:\n",
    "- See available tools and their schemas\n",
    "- Decide when to call tools\n",
    "- Generate proper tool call arguments\n",
    "\"\"\"\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "# =============================================================================\n",
    "# Method 1: @tool Decorator (Recommended for Most Cases)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "The @tool decorator automatically:\n",
    "- Extracts the function name as tool name\n",
    "- Uses the docstring as tool description\n",
    "- Generates schema from type hints\n",
    "\"\"\"\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the current weather for a given location.\n",
    "    \n",
    "    Args:\n",
    "        location: The city or location to get weather for (e.g., \"New York\", \"London\")\n",
    "    \n",
    "    Returns:\n",
    "        A string describing the current weather conditions\n",
    "    \"\"\"\n",
    "    # In production, this would call a real weather API\n",
    "    return f\"The weather in {location} is sunny with a high of 75Â°F.\"\n",
    "\n",
    "@tool\n",
    "def search_database(query: str, limit: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search the database for relevant information.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        limit: Maximum number of results to return (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        Search results as a formatted string\n",
    "    \"\"\"\n",
    "    return f\"Found {limit} results for '{query}': [Result 1, Result 2, ...]\"\n",
    "\n",
    "# Check the tool schema\n",
    "print(\"=\" * 60)\n",
    "print(\"Tool Schema (auto-generated from type hints):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Name: {get_weather.name}\")\n",
    "print(f\"Description: {get_weather.description}\")\n",
    "print(f\"Args Schema: {get_weather.args_schema.schema() if get_weather.args_schema else 'None'}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Method 2: Using Pydantic for Complex Schemas\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "For tools with complex input validation, use a Pydantic model\n",
    "with the args_schema parameter.\n",
    "\"\"\"\n",
    "\n",
    "class WeatherInput(BaseModel):\n",
    "    \"\"\"Input schema for weather tool with validation.\"\"\"\n",
    "    location: str = Field(..., description=\"City name or location\", min_length=2)\n",
    "    units: Optional[str] = Field(default=\"fahrenheit\", description=\"Temperature units: 'fahrenheit' or 'celsius'\")\n",
    "\n",
    "@tool(args_schema=WeatherInput)\n",
    "def get_weather_detailed(location: str, units: str = \"fahrenheit\") -> str:\n",
    "    \"\"\"Get detailed weather with unit selection.\"\"\"\n",
    "    temp = 75 if units == \"fahrenheit\" else 24\n",
    "    return f\"Weather in {location}: {temp}Â°{'F' if units == 'fahrenheit' else 'C'}, sunny\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Tool with Pydantic Schema:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Schema: {get_weather_detailed.args_schema.schema()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "q1cdcg6w2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Model with Tools Bound\n",
      "============================================================\n",
      "Original model: ChatOpenAI\n",
      "Model with tools: RunnableBinding\n",
      "Tools available: ['get_weather', 'search_database', 'get_weather_detailed']\n",
      "\n",
      "============================================================\n",
      "Tool Call Response Analysis\n",
      "============================================================\n",
      "Response type: AIMessage\n",
      "Has tool calls: True\n",
      "\n",
      "Tool calls made:\n",
      "  - Tool: get_weather\n",
      "    Args: {'location': 'San Francisco'}\n",
      "    ID: call_bWqGG0Y1W3iS1MpOb0pyILNq\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Binding Tools to Models (LangChain 1.0+)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "bind_tools() - Attaching Tools to LLMs\n",
    "--------------------------------------\n",
    "The bind_tools() method is how you give an LLM access to tools.\n",
    "It returns a new model instance with tools attached.\n",
    "\n",
    "Supported Providers (2025):\n",
    "- OpenAI (gpt-4o, gpt-4o-mini, gpt-4-turbo)\n",
    "- Anthropic (claude-3.5-sonnet, claude-3-opus)\n",
    "- Google (gemini-1.5-pro, gemini-1.5-flash)\n",
    "- Groq (llama-3.1-70b, mixtral-8x7b)\n",
    "- Mistral, Cohere, and more\n",
    "\n",
    "Important: bind_tools() creates a NEW model instance\n",
    "           The original model is unchanged\n",
    "\"\"\"\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "# Initialize model\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0)\n",
    "\n",
    "# Define tools\n",
    "tools = [get_weather, search_database, get_weather_detailed]\n",
    "\n",
    "# Bind tools to model - creates a new model with tool access\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Model with Tools Bound\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original model: {type(model).__name__}\")\n",
    "print(f\"Model with tools: {type(model_with_tools).__name__}\")\n",
    "print(f\"Tools available: {[t.name for t in tools]}\")\n",
    "\n",
    "# =============================================================================\n",
    "# How Tool Calling Works\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "When you invoke a model with bound tools:\n",
    "\n",
    "1. LLM sees the user's question + available tool schemas\n",
    "2. LLM decides IF it needs to use a tool\n",
    "3. If yes: Returns AIMessage with tool_calls populated\n",
    "4. If no: Returns regular AIMessage with content\n",
    "\n",
    "The tool_calls are structured as:\n",
    "{\n",
    "    \"name\": \"tool_name\",\n",
    "    \"args\": {\"arg1\": \"value1\", \"arg2\": \"value2\"},\n",
    "    \"id\": \"unique_call_id\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Example: Ask something that requires a tool\n",
    "response = model_with_tools.invoke([\n",
    "    HumanMessage(content=\"What's the weather in San Francisco?\")\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Tool Call Response Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Response type: {type(response).__name__}\")\n",
    "print(f\"Has tool calls: {bool(response.tool_calls)}\")\n",
    "if response.tool_calls:\n",
    "    print(f\"\\nTool calls made:\")\n",
    "    for tc in response.tool_calls:\n",
    "        print(f\"  - Tool: {tc['name']}\")\n",
    "        print(f\"    Args: {tc['args']}\")\n",
    "        print(f\"    ID: {tc['id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c24615qjgk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Complete Tool Execution Flow\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Step 1: User asks question\n",
      "   Message: What's the weather in Tokyo?\n",
      "\n",
      "ðŸ”§ Step 2: Model decides to use tool\n",
      "   Tool: get_weather\n",
      "   Args: {'location': 'Tokyo'}\n",
      "\n",
      "ðŸ“Š Step 3: Tool execution result\n",
      "   Result: The weather in Tokyo is sunny with a high of 75Â°F.\n",
      "\n",
      "âœ… Step 4: Model generates final answer\n",
      "   Answer: The weather in Tokyo is sunny with a high of 75Â°F.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Executing Tools and Getting Final Answers\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Complete Tool Execution Flow:\n",
    "-----------------------------\n",
    "1. User asks a question\n",
    "2. Model returns AIMessage with tool_calls\n",
    "3. YOU execute the tool with the provided args\n",
    "4. Send ToolMessage with the result back to the model\n",
    "5. Model generates final answer using the tool result\n",
    "\n",
    "Message Flow:\n",
    "-------------\n",
    "    [HumanMessage]     \"What's the weather in NYC?\"\n",
    "           â”‚\n",
    "           â–¼\n",
    "    [AIMessage]        tool_calls: [{name: \"get_weather\", args: {location: \"NYC\"}}]\n",
    "           â”‚\n",
    "           â–¼ (You execute the tool)\n",
    "    [ToolMessage]      \"The weather in NYC is sunny, 75Â°F\"\n",
    "           â”‚\n",
    "           â–¼\n",
    "    [AIMessage]        \"The weather in New York City is currently sunny...\"\n",
    "\"\"\"\n",
    "\n",
    "def execute_tools(tool_calls: list, available_tools: dict) -> list:\n",
    "    \"\"\"\n",
    "    Execute tool calls and return ToolMessages with results.\n",
    "    \n",
    "    Args:\n",
    "        tool_calls: List of tool calls from AIMessage\n",
    "        available_tools: Dict mapping tool names to tool functions\n",
    "    \n",
    "    Returns:\n",
    "        List of ToolMessage objects with results\n",
    "    \"\"\"\n",
    "    tool_messages = []\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        tool_id = tool_call[\"id\"]\n",
    "        \n",
    "        # Find and execute the tool\n",
    "        if tool_name in available_tools:\n",
    "            result = available_tools[tool_name].invoke(tool_args)\n",
    "        else:\n",
    "            result = f\"Error: Tool '{tool_name}' not found\"\n",
    "        \n",
    "        # Create ToolMessage with the result\n",
    "        tool_messages.append(ToolMessage(\n",
    "            content=str(result),\n",
    "            tool_call_id=tool_id  # Must match the original tool call ID\n",
    "        ))\n",
    "    \n",
    "    return tool_messages\n",
    "\n",
    "# Create tool lookup dictionary\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "# Complete conversation with tool execution\n",
    "messages = [HumanMessage(content=\"What's the weather in Tokyo?\")]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Complete Tool Execution Flow\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Get tool call from model\n",
    "print(\"\\nðŸ“ Step 1: User asks question\")\n",
    "print(f\"   Message: {messages[0].content}\")\n",
    "\n",
    "response = model_with_tools.invoke(messages)\n",
    "messages.append(response)\n",
    "\n",
    "# Step 2: Execute tools\n",
    "print(\"\\nðŸ”§ Step 2: Model decides to use tool\")\n",
    "if response.tool_calls:\n",
    "    for tc in response.tool_calls:\n",
    "        print(f\"   Tool: {tc['name']}\")\n",
    "        print(f\"   Args: {tc['args']}\")\n",
    "    \n",
    "    # Execute and add results\n",
    "    tool_results = execute_tools(response.tool_calls, tools_by_name)\n",
    "    messages.extend(tool_results)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Step 3: Tool execution result\")\n",
    "    for tm in tool_results:\n",
    "        print(f\"   Result: {tm.content}\")\n",
    "    \n",
    "    # Step 4: Get final answer\n",
    "    final_response = model_with_tools.invoke(messages)\n",
    "    \n",
    "    print(\"\\nâœ… Step 4: Model generates final answer\")\n",
    "    print(f\"   Answer: {final_response.content}\")\n",
    "else:\n",
    "    print(\"   No tools needed for this question\")\n",
    "    print(f\"   Direct answer: {response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "zfvmm57j50p",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Agent Loop Example\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Question: What's the weather in Paris?\n",
      "----------------------------------------\n",
      "  ðŸ”§ Iteration 1: Executing 1 tool(s)\n",
      "     - get_weather({'location': 'Paris'})\n",
      "âœ… Answer: The weather in Paris is sunny with a high of 75Â°F.\n",
      "\n",
      "ðŸ“ Question: Search for information about Python programming\n",
      "----------------------------------------\n",
      "  ðŸ”§ Iteration 1: Executing 1 tool(s)\n",
      "     - search_database({'query': 'Python programming'})\n",
      "âœ… Answer: I found some information about Python programming. Here are the results:\n",
      "\n",
      "1. **Result 1**: Overview of Python programming language, its features, and applications.\n",
      "2. **Result 2**: A guide on how to g...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Agent Loop with Tools (LangChain 1.0+ Pattern)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Building a Simple Agent Loop\n",
    "----------------------------\n",
    "An agent is just a loop that:\n",
    "1. Takes user input\n",
    "2. Calls LLM with tools\n",
    "3. Executes any tool calls\n",
    "4. Repeats until no more tool calls\n",
    "5. Returns final answer\n",
    "\n",
    "This is the foundation of how agents work in LangChain/LangGraph.\n",
    "\"\"\"\n",
    "\n",
    "def run_agent(user_input: str, max_iterations: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Simple agent loop that handles tool calling.\n",
    "    \n",
    "    Args:\n",
    "        user_input: The user's question\n",
    "        max_iterations: Maximum tool execution cycles (safety limit)\n",
    "    \n",
    "    Returns:\n",
    "        The agent's final response\n",
    "    \"\"\"\n",
    "    messages = [HumanMessage(content=user_input)]\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        # Get model response\n",
    "        response = model_with_tools.invoke(messages)\n",
    "        messages.append(response)\n",
    "        \n",
    "        # Check if model wants to use tools\n",
    "        if not response.tool_calls:\n",
    "            # No tool calls = we have our final answer\n",
    "            return response.content\n",
    "        \n",
    "        # Execute all tool calls\n",
    "        print(f\"  ðŸ”§ Iteration {i+1}: Executing {len(response.tool_calls)} tool(s)\")\n",
    "        for tc in response.tool_calls:\n",
    "            print(f\"     - {tc['name']}({tc['args']})\")\n",
    "        \n",
    "        tool_results = execute_tools(response.tool_calls, tools_by_name)\n",
    "        messages.extend(tool_results)\n",
    "    \n",
    "    # Safety: return last response if we hit max iterations\n",
    "    return messages[-1].content if messages else \"Max iterations reached\"\n",
    "\n",
    "# Test the agent\n",
    "print(\"=\" * 60)\n",
    "print(\"Agent Loop Example\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "questions = [\n",
    "    \"What's the weather in Paris?\",\n",
    "    \"Search for information about Python programming\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\nðŸ“ Question: {q}\")\n",
    "    print(\"-\" * 40)\n",
    "    answer = run_agent(q)\n",
    "    print(f\"âœ… Answer: {answer[:200]}...\" if len(answer) > 200 else f\"âœ… Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5x9l7eh2v7p",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Tool Choice Examples\n",
      "============================================================\n",
      "\n",
      "ðŸ“Œ Forced to use 'get_weather' tool:\n",
      "   Tool called: get_weather\n",
      "   Args: {'location': 'machine learning'}\n",
      "\n",
      "ðŸ“Œ Required to use a tool (any tool):\n",
      "   Tool called: get_weather\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Advanced: Tool Choice and Forcing Tool Usage\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Controlling Tool Selection\n",
    "--------------------------\n",
    "bind_tools() accepts a tool_choice parameter to control how the model\n",
    "uses tools:\n",
    "\n",
    "Options:\n",
    "- \"auto\" (default): Model decides when to use tools\n",
    "- \"any\" / \"required\": Model MUST use at least one tool\n",
    "- \"none\": Model cannot use tools (even if bound)\n",
    "- {\"type\": \"tool\", \"name\": \"tool_name\"}: Force specific tool\n",
    "\n",
    "Use Cases:\n",
    "- tool_choice=\"required\": Ensure tool is always called\n",
    "- tool_choice={\"type\": \"tool\", \"name\": \"get_weather\"}: Force weather lookup\n",
    "- tool_choice=\"none\": Temporarily disable tools\n",
    "\"\"\"\n",
    "\n",
    "# Force model to use a specific tool\n",
    "model_forced_weather = model.bind_tools(\n",
    "    tools,\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}\n",
    ")\n",
    "\n",
    "# Force model to use ANY tool (but must use one)\n",
    "model_must_use_tool = model.bind_tools(\n",
    "    tools,\n",
    "    tool_choice=\"required\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Tool Choice Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example 1: Forced to use get_weather\n",
    "print(\"\\nðŸ“Œ Forced to use 'get_weather' tool:\")\n",
    "response = model_forced_weather.invoke([\n",
    "    HumanMessage(content=\"Tell me about machine learning\")  # Unrelated question!\n",
    "])\n",
    "if response.tool_calls:\n",
    "    print(f\"   Tool called: {response.tool_calls[0]['name']}\")\n",
    "    print(f\"   Args: {response.tool_calls[0]['args']}\")\n",
    "else:\n",
    "    print(f\"   Direct response: {response.content[:100]}...\")\n",
    "\n",
    "# Example 2: Must use some tool\n",
    "print(\"\\nðŸ“Œ Required to use a tool (any tool):\")\n",
    "response = model_must_use_tool.invoke([\n",
    "    HumanMessage(content=\"What's happening in London today?\")\n",
    "])\n",
    "if response.tool_calls:\n",
    "    print(f\"   Tool called: {response.tool_calls[0]['name']}\")\n",
    "else:\n",
    "    print(\"   No tool called (unexpected)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gk9pu6bwlqa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Tools Module Complete!\n",
      "============================================================\n",
      "\n",
      "Next Steps:\n",
      "-----------\n",
      "1. 4-messages.ipynb - Message types and conversation management\n",
      "2. 5-structureoutput.ipynb - Getting structured data from LLMs\n",
      "3. 6-middleware.ipynb - Request/response middleware\n",
      "\n",
      "For more complex agent patterns, see:\n",
      "- LangGraph for stateful, multi-step agents\n",
      "- projects2/5agent-guardrails.py for agent with constraints\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Summary: Tools in LangChain 1.0+\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "=====================================================================\n",
    "KEY TAKEAWAYS - Tools in LangChain\n",
    "=====================================================================\n",
    "\n",
    "1. CREATING TOOLS:\n",
    "   ----------------\n",
    "   @tool\n",
    "   def my_tool(arg: str) -> str:\n",
    "       '''Docstring becomes tool description'''\n",
    "       return result\n",
    "\n",
    "   # With Pydantic validation:\n",
    "   @tool(args_schema=MyInputSchema)\n",
    "   def validated_tool(arg: str) -> str: ...\n",
    "\n",
    "2. BINDING TOOLS:\n",
    "   ---------------\n",
    "   model_with_tools = model.bind_tools([tool1, tool2])\n",
    "   \n",
    "   # With tool choice:\n",
    "   model.bind_tools(tools, tool_choice=\"required\")\n",
    "   model.bind_tools(tools, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"my_tool\"}})\n",
    "\n",
    "3. TOOL EXECUTION FLOW:\n",
    "   ---------------------\n",
    "   response = model_with_tools.invoke(messages)\n",
    "   \n",
    "   if response.tool_calls:\n",
    "       for tc in response.tool_calls:\n",
    "           result = tools_dict[tc[\"name\"]].invoke(tc[\"args\"])\n",
    "           messages.append(ToolMessage(content=result, tool_call_id=tc[\"id\"]))\n",
    "       \n",
    "       final = model_with_tools.invoke(messages)\n",
    "\n",
    "4. AGENT LOOP PATTERN:\n",
    "   --------------------\n",
    "   while True:\n",
    "       response = model_with_tools.invoke(messages)\n",
    "       if not response.tool_calls:\n",
    "           break\n",
    "       # Execute tools, add results to messages\n",
    "\n",
    "5. BEST PRACTICES:\n",
    "   ----------------\n",
    "   - Write clear docstrings (they become tool descriptions)\n",
    "   - Use type hints (they define the schema)\n",
    "   - Use Pydantic for complex validation\n",
    "   - Always include tool_call_id in ToolMessage\n",
    "   - Set max_iterations to prevent infinite loops\n",
    "   - Use temperature=0 for consistent tool calling\n",
    "\n",
    "Common Imports:\n",
    "---------------\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain.chat_models import init_chat_model\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "=====================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Tools Module Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Next Steps:\n",
    "-----------\n",
    "1. 4-messages.ipynb - Message types and conversation management\n",
    "2. 5-structureoutput.ipynb - Getting structured data from LLMs\n",
    "3. 6-middleware.ipynb - Request/response middleware\n",
    "\n",
    "For more complex agent patterns, see:\n",
    "- LangGraph for stateful, multi-step agents\n",
    "- projects2/5agent-guardrails.py for agent with constraints\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
